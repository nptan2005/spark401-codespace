# Phase 2: GCP Free Tier - DataProc Serverless

* Dataproc Serverless
* GCS (Bronze / Silver)
* BigQuery (Gold)
* Airflow ch·ªâ submit job

## Plan test:
```text
GCS Bronze (CSV)
   ‚Üì
Dataproc Serverless (PySpark)
   ‚Üì
GCS Silver (Parquet + partition)
   ‚Üì
Dataproc Serverless
   ‚Üì
BigQuery Gold
```

## B∆Ø·ªöC 1: QUY ∆Ø·ªöC ENTERPRISE 

### 1Ô∏è‚É£ Naming convention (gi·ªØ t·ª´ gi·ªù):

|Th√†nh ph·∫ßn|Quy ∆∞·ªõc|
|----------|-------|
|Project|cdp-dem-project|
|Region|asia-southeast1|
|GCS|cdp-{env}-{layer}|
|Dataset|{env}_{layer}|
|DAG|{layer}_to_{layer}|

> üëâ env = dev (free tier)

### 2Ô∏è‚É£ Bucket

T√°ch bucket kh√¥ng d√πng chung

```text
gs://cdp-dev-bronze/
gs://cdp-dev-silver/
gs://cdp-dev-gold/
gs://cdp-dev-jobs/
```
> ‚ùó JOB_BUCKET ‚â† BRONZE_BUCKET

## B∆Ø·ªöC 2: Phase n√†y kh√¥ng d√πng cluster, n√™n s·∫Ω xo√° bi·∫øn cluster:

```bash
airflow variables delete DATAPROC_CLUSTER
```
## B∆Ø·ªöC 3: DAG CHU·∫®N SERVERLESS (ENTERPRISE STYLE):

**üß± Bronze ‚Üí Silver (Dataproc Serverless**

```python
from airflow import DAG
from airflow.providers.google.cloud.operators.dataproc import DataprocCreateBatchOperator
from airflow.models import Variable
from datetime import datetime

PROJECT_ID = Variable.get("PROJECT_ID")
REGION = Variable.get("REGION")

JOB_BUCKET = Variable.get("JOB_BUCKET")
BRONZE_PATH = Variable.get("BRONZE_PATH")
SILVER_PATH = Variable.get("SILVER_PATH")

with DAG(
    dag_id="bronze_to_silver_gcp",
    start_date=datetime(2025, 12, 12),
    schedule=None,
    catchup=False,
    tags=["cdp", "serverless", "bronze", "silver"],
) as dag:

    DataprocCreateBatchOperator(
        task_id="bronze_to_silver_gcp",
        project_id=PROJECT_ID,
        region=REGION,
        batch={
            "pyspark_batch": {
                "main_python_file_uri": f"gs://{JOB_BUCKET}/jobs/bronze_to_silver.py",
                "args": [BRONZE_PATH, SILVER_PATH],
            },
        },
    )
```

**üß± Silver ‚Üí Gold (BigQuery)**
```python
from airflow import DAG
from airflow.providers.google.cloud.operators.dataproc import DataprocCreateBatchOperator
from airflow.models import Variable
from datetime import datetime

PROJECT_ID = Variable.get("PROJECT_ID")
REGION = Variable.get("REGION")

JOB_BUCKET = Variable.get("JOB_BUCKET")
SILVER_PATH = Variable.get("SILVER_PATH")
BQ_DATASET = Variable.get("BQ_DATASET")
BQ_TABLE = Variable.get("BQ_TABLE")

with DAG(
    dag_id="silver_to_gold_gcp",
    start_date=datetime(2025, 12, 12),
    schedule=None,
    catchup=False,
    tags=["cdp", "serverless", "gold"],
) as dag:

    DataprocCreateBatchOperator(
        task_id="silver_to_gold_gcp",
        project_id=PROJECT_ID,
        region=REGION,
        batch={
            "pyspark_batch": {
                "main_python_file_uri": f"gs://{JOB_BUCKET}/jobs/silver_to_gold.py",
                "args": [SILVER_PATH, PROJECT_ID, BQ_DATASET, BQ_TABLE],
            },
        },
    )
```

## B∆Ø·ªöC 4: CHECK QUY·ªÄN (ENTERPRISE B·∫ÆT BU·ªòC):

Dataproc Serverless c·∫ßn IAM sau:
```text
roles/dataproc.editor
roles/storage.objectAdmin
roles/bigquery.dataEditor
roles/bigquery.jobUser
```

check:
```bash
gcloud projects get-iam-policy cdp-dem-project
```

### üí∞ FREE TIER ‚Äì C√ÅCH KH√îNG B·ªä ƒê·ªêT TI·ªÄN:

|M·ª•c|C√°ch|
|---|----|
|Dataproc|Serverless only|
|VM|‚ùå Kh√¥ng d√πng|
|Composer|Ch∆∞a t·∫°o|
|Job|Manual trigger|
|Logs|Gi·ªØ m·∫∑c ƒë·ªãnh|
> üëâ M·ªói DAG run ch·ªâ v√†i cent

## B∆∞·ªõc 5: VALIDATE + CHU·∫®N HO√Å ENTERPRISE FLOW (SERVERLESS)

### 5.1 jobs/bronze_to_silver.py
```python
import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, current_timestamp, to_timestamp
)
from pyspark.sql.types import (
    StructType, StructField,
    StringType, IntegerType, DoubleType, TimestampType
)


def get_spark(app_name: str) -> SparkSession:
    return (
        SparkSession.builder
        .appName(app_name)
        .getOrCreate()
    )


def read_bronze(spark: SparkSession, path: str):
    return (
        spark.read
        .option("header", "true")
        .csv(path)
    )


def transform_to_silver(df):
    return (
        df
        .withColumn("order_id", col("order_id").cast(IntegerType()))
        .withColumn("customer_id", col("customer_id").cast(IntegerType()))
        .withColumn("amount", col("amount").cast(DoubleType()))
        .withColumn(
            "order_ts",
            to_timestamp("order_ts", "yyyy-MM-dd HH:mm:ss")
        )
        # RULE: Silver kh√¥ng cho amount null
        .filter(col("amount").isNotNull())
        # audit columns
        .withColumn("processed_at", current_timestamp())
    )


def write_silver(df, path: str):
    (
        df.write \
        .mode("overwrite") \
        .partitionBy("order_date") \
        .parquet(path)
    )


def main(bronze_path: str, silver_path: str):
    spark = get_spark("bronze-to-silver")

    df_bronze = read_bronze(spark, bronze_path)
    df_silver = transform_to_silver(df_bronze)

    write_silver(df_silver, silver_path)

    spark.stop()


if __name__ == "__main__":
    if len(sys.argv) != 3:
        print(
            "Usage: spark-submit bronze_to_silver.py "
            "<bronze_path> <silver_path>"
        )
        sys.exit(1)

    bronze_path = sys.argv[1]
    silver_path = sys.argv[2]

    main(bronze_path, silver_path)
```

#### Test job Bronze_to_silver:

T·∫°o folder:
```bash
mkdir -p data/bronze/orders
mkdir -p data/silver/orders
```

Data:
```csv
order_id,customer_id,amount,order_ts,currency
1,101,100.5,2025-12-10 10:00:00,VND
2,102,,2025-12-10 11:00:00,VND
3,103,200.0,2025-12-11 09:30:00,VND
```

run job:
```bash
spark-submit \
  jobs/bronze_to_silver.py \
  data/bronze/orders \
  data/silver/orders
```

### 5.2 jobs/silver_to_gold.py
```python
import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date


def get_spark(app_name: str) -> SparkSession:
    return (
        SparkSession.builder
        .appName(app_name)
        # BigQuery connector c√≥ s·∫µn tr√™n Dataproc
        .getOrCreate()
    )


def read_silver(spark: SparkSession, path: str):
    return spark.read.parquet(path)


def transform_to_gold(df):
    return (
        df
        .withColumn("order_date", to_date(col("order_ts")))
        .select(
            "order_id",
            "customer_id",
            "amount",
            "currency",
            "order_ts",
            "order_date"
        )
    )


def write_gold(df, project: str, dataset: str, table: str):
    (
        df.write \
        .format("bigquery") \
        .option("table", f"{project}:{dataset}.{table}") \
        .option("temporaryGcsBucket", "cdp-dem-bq-temp") \
        .mode("overwrite") \
        .save()
    )


def main(silver_path, project, dataset, table):
    spark = get_spark("silver-to-gold")

    df_silver = read_silver(spark, silver_path)
    df_gold = transform_to_gold(df_silver)

    write_gold(df_gold, project, dataset, table)

    # test
    # df_gold.show()
    # df_gold.printSchema()

    spark.stop()


if __name__ == "__main__":
    if len(sys.argv) != 5:
        print(
            "Usage: spark-submit silver_to_gold.py "
            "<silver_path> <project> <dataset> <table>"
        )
        sys.exit(1)

    silver_path = sys.argv[1]
    project = sys.argv[2]
    dataset = sys.argv[3]
    table = sys.argv[4]

    main(silver_path, project, dataset, table)
```

#### Test silver_to_gold:
Test local thay write_to_gold b·∫±ng:
```python
df_gold.show()
df_gold.printSchema()
```

```bash
spark-submit \
  jobs/silver_to_gold.py \
  data/silver/orders \
  dummy_project dummy_dataset dummy_table
```
### 5.3 Test job to GCP

#### 1Ô∏è‚É£ Ki·ªÉm tra Bronze data c√≥ th·∫≠t ch∆∞a
```bash
gsutil ls gs://cdp-dem-bronze/orders/
```

#### 2Ô∏è‚É£ Xem th·ª≠ n·ªôi dung Bronze
```bash
gsutil cat gs://cdp-dem-bronze/orders/*.csv | head
```

### 5.4 CH·∫†Y BRONZE ‚Üí SILVER (SERVERLESS):

#### ‚úÖ STEP 5.4.1 ‚Äì UPLOAD PYSPARK JOB L√äN GCS:
```bash
gsutil cp jobs/bronze_to_silver.py gs://cdp-dem-bronze/jobs/
```
```bash
gsutil cp jobs/silver_to_gold.py gs://cdp-dem-bronze/jobs/
```
Ki·ªÖm tra:
```bash
gsutil ls gs://cdp-dem-bronze/jobs/
```

#### ‚úÖ STEP 5.4.2 ‚Äì CH·∫†Y L·∫†I SERVERLESS

```bash
gcloud dataproc batches submit pyspark \
  gs://cdp-dem-bronze/jobs/bronze_to_silver.py \
  --region asia-southeast1 \
  -- \
  gs://cdp-dem-bronze/orders \
  gs://cdp-dem-silver/orders
```

#### üìå Gi·∫£i th√≠ch ng·∫Øn:
	*	batches submit = Dataproc Serverless
	*	-- = b·∫Øt ƒë·∫ßu truy·ªÅn sys.argv
	*	2 path = ƒë√∫ng v·ªõi main(bronze_path, silver_path)

#### ‚úÖ STEP 5.4 ‚Äì KI·ªÇM TRA SILVER
```bash
gsutil ls gs://cdp-dem-silver/orders/
```

#### Gi·∫£i th√≠ch job:
|Th√†nh ph·∫ßn|Vai tr√≤|
|----------|-------|
|Codespace|Vi·∫øt code|
|GCS|L∆∞u data + job|
|Dataproc Serverless|Ch·∫°y Spark|
|BigQuery|Gold layer|
> üëâ Serverless = kh√¥ng cluster = ƒë√∫ng h∆∞·ªõng enterprise + free tier

### STEP 5.5 ‚Äì CH·∫†Y SILVER ‚Üí GOLD (BIGQUERY): 

```bash
gcloud dataproc batches submit pyspark \
  gs://cdp-dem-bronze/jobs/silver_to_gold.py \
  --region asia-southeast1 \
  -- \
  gs://cdp-dem-silver/orders \
  cdp-dem-project \
  cdp_gold \
  orders
```
Check Batch:
```bash
gcloud dataproc batches list --region asia-southeast1
```
### STEP 5.6 ‚Äì KI·ªÇM TRA BIGQUERY:

```bash
bq ls cdp-dem-project:cdp_gold
```

```bash
bq show cdp-dem-project:cdp_gold.orders
```

```bash
bq query --nouse_legacy_sql \
'SELECT COUNT(*) FROM `cdp-dem-project.cdp_gold.orders`'
```

#### ƒê·∫øn phase 2 - B∆∞·ªõc 5:
	‚Ä¢	‚ùå Kh√¥ng c·∫ßn Dataproc cluster
	‚Ä¢	‚úÖ D√πng Spark Serverless
	‚Ä¢	‚úÖ Spark job run

#### üß† NGUY√äN T·∫ÆC ENTERPRISE:

|Layer|Rule|
|-----|----|
|Bronze|raw, kh√¥ng rule|
|Silver|clean + audit|
|Gold|business, aggregation|
