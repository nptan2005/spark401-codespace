# Phase 3: Airflow orchestration

## Overview:
1.	Refactor job â†’ idempotent
2.	Airflow DAG trigger Dataproc Serverless
3.	Data dependency (Bronze â†’ Silver â†’ Gold)
4.	Retry / alert / logging
5.	Dev (Codespace) â†’ Prod (Composer)

## ğŸ¯ Step 1 â€“ Chuáº©n hoÃ¡ job cho Airflow
Airflow cháº¡y theo schedule â†’ cáº§n incremental

### âœ… 1.1: CHá»T NGUYÃŠN Táº®C ENTERPRISE

#### Silver:
*	Partition by order_date
*	CÃ³ thá»ƒ overwrite theo partition

#### Gold:
*	KhÃ´ng overwrite toÃ n báº£ng
*	Load theo ngÃ y (partition)

### 1.2 âœï¸ Sá»¬A silver_to_gold.py:

#### Ná»™i dung edit
```python
def write_gold(df, project, dataset, table):
    (
        df.write
        .format("bigquery")
        .option("table", f"{project}:{dataset}.{table}")
        .option("temporaryGcsBucket", "cdp-dem-bq-temp")
        .option("partitionField", "order_date")   # ğŸ‘ˆ QUAN TRá»ŒNG
        .option("partitionType", "DAY")
        .mode("append")                            # ğŸ‘ˆ KHÃ”NG overwrite
        .save()
    )
```

#### full file:
```python
import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date


def get_spark(app_name: str) -> SparkSession:
    return (
        SparkSession.builder
        .appName(app_name)
        # BigQuery connector cÃ³ sáºµn trÃªn Dataproc
        .getOrCreate()
    )


def read_silver(spark: SparkSession, path: str):
    return spark.read.parquet(path)


def transform_to_gold(df):
    return (
        df
        .withColumn("order_date", to_date(col("order_ts")))
        .select(
            "order_id",
            "customer_id",
            "amount",
            "currency",
            "order_ts",
            "order_date"
        )
    )

def write_gold(df, project, dataset, table):
    (
        df.write
        .format("bigquery")
        .option("table", f"{project}:{dataset}.{table}")
        .option("temporaryGcsBucket", "cdp-dem-bq-temp")
        .option("partitionField", "order_date")   
        .option("partitionType", "DAY")
        .mode("append")                            
        .save()
    )

def main(silver_path, project, dataset, table):
    spark = get_spark("silver-to-gold")

    df_silver = read_silver(spark, silver_path)
    df_gold = transform_to_gold(df_silver)

    write_gold(df_gold, project, dataset, table)

    # test
    # df_gold.show()
    # df_gold.printSchema()

    spark.stop()


if __name__ == "__main__":
    if len(sys.argv) != 5:
        print(
            "Usage: spark-submit silver_to_gold.py "
            "<silver_path> <project> <dataset> <table>"
        )
        sys.exit(1)

    silver_path = sys.argv[1]
    project = sys.argv[2]
    dataset = sys.argv[3]
    table = sys.argv[4]

    main(silver_path, project, dataset, table)
```

### 1.3 Test code:

1ï¸âƒ£ Upload láº¡i job
```bash
gsutil cp jobs/silver_to_gold.py gs://cdp-dem-bronze/jobs/
```
**Cáº§n remove table cÅ©, do table trÆ°á»›c Ä‘Ã³ ko cÃ³ partition:**
```bash
bq rm -f -t cdp-dem-project:cdp_gold.orders
```
2ï¸âƒ£ Cháº¡y láº¡i batch:
```bash
gcloud dataproc batches submit pyspark \
  gs://cdp-dem-bronze/jobs/silver_to_gold.py \
  --region asia-southeast1 \
  -- \
  gs://cdp-dem-silver/orders \
  cdp-dem-project \
  cdp_gold \
  orders
```
3ï¸âƒ£ Check BigQuery
```bash
bq show cdp-dem-project:cdp_gold.orders
```

## Step 2: New DAG: 

**Má»¥c tiÃªu:***
Codespace chá»‰ Ä‘á»ƒ DEV
Airflow chá»‰ Ä‘á»ƒ ORCHESTRATE
Spark cháº¡y trÃªn Dataproc Serverless

**Kiáº¿n trÃºc**
```text
Airflow DAG
   |
   |-- bronze_to_silver (Dataproc Serverless)
   |
   |-- silver_to_gold   (Dataproc Serverless)
```
**ThÃ nh pháº§n:**
|ThÃ nh pháº§n|Vai trÃ²|
|----------|-------|
|Codespace|Dev DAG|
|Airflow local|Validate DAG|
|Dataproc Serverless|Run Spark|
|BigQuery|Serving|
|Composer|Prod (sau)|


### 2.1 ğŸ“ dags/bronze_to_gold_dag.py:

```python
from airflow import DAG
from airflow.providers.google.cloud.operators.dataproc import (
    DataprocCreateBatchOperator
)
from airflow.models import Variable
from datetime import datetime

PROJECT_ID = Variable.get("PROJECT_ID")
REGION = Variable.get("REGION")

BRONZE_PATH = Variable.get("BRONZE_PATH")
SILVER_PATH = Variable.get("SILVER_PATH")

BQ_DATASET = Variable.get("BQ_DATASET")
BQ_TABLE = Variable.get("BQ_TABLE")

JOB_BUCKET = Variable.get("JOB_BUCKET")

with DAG(
    dag_id="bronze_to_gold_dataproc_serverless",
    start_date=datetime(2025, 12, 12),
    schedule=None,
    catchup=False,
    tags=["cdp", "dataproc", "serverless"],
) as dag:

    bronze_to_silver = DataprocCreateBatchOperator(
        task_id="bronze_to_silver",
        project_id=PROJECT_ID,
        region=REGION,
        batch={
            "pysparkBatch": {
                "mainPythonFileUri": f"gs://{JOB_BUCKET}/jobs/bronze_to_silver.py",
                "args": [BRONZE_PATH, SILVER_PATH],
            }
        },
    )

    silver_to_gold = DataprocCreateBatchOperator(
        task_id="silver_to_gold",
        project_id=PROJECT_ID,
        region=REGION,
        batch={
            "pysparkBatch": {
                "mainPythonFileUri": f"gs://{JOB_BUCKET}/jobs/silver_to_gold.py",
                "args": [
                    SILVER_PATH,
                    PROJECT_ID,
                    BQ_DATASET,
                    BQ_TABLE,
                ],
            }
        },
    )

    bronze_to_silver >> silver_to_gold
```

### 2.2 TEST LOCAL (CODESPACE):

```bash
airflow dags list | grep bronze_to_gold
airflow dags test bronze_to_gold_dataproc_serverless 2025-12-12
```
**ğŸ‘‰ Airflow sáº½:**
*	KhÃ´ng cháº¡y Spark local
*	Chá»‰ submit Dataproc Serverless batch

## Step 3: Chuáº©n hoÃ¡ Airflow Dev (Professional / Bank-grade):

**<ins>Má»¥c tiÃªu:</ins>**
*	KhÃ´ng Ä‘á»ƒ example DAG
*	KhÃ´ng láº«n DAG local / DAG prod
*	Chuáº©n bá»‹ CI/CD Ä‘áº©y lÃªn Cloud Composer

### 3.1 Táº®T EXAMPLE DAG:

Táº¯t:
```bash
export AIRFLOW__CORE__LOAD_EXAMPLES=False
```

Check:
```bash
airflow dags list | grep example
```

### 3.2 Cáº¤U TRÃšC DAG CHUáº¨N:

Backup dags dev and change name:
```bash
mkdir -p ./.airflow/backup_dags
mv ./.airflow/dags/* ./.airflow/backup_dags
cp ./.airflow/backup_dags/bronze_to_gold_dataproc_serverless.py ./.airflow/dags/
```
Cáº¥u trÃºc:
```text
((.venv) ) @nptan2005 âœ /workspaces/spark401-codespace (main) $ tree -L4 ./.airflow/
./.airflow/
â”œâ”€â”€ README.md
â”œâ”€â”€ airflow.cfg
â”œâ”€â”€ airflow.db
â”œâ”€â”€ backup_dags
â”‚   â”œâ”€â”€ bronze_to_gold_dataproc_serverless.py
â”‚   â”œâ”€â”€ bronze_to_silver_dag.py
â”‚   â”œâ”€â”€ bronze_to_silver_gcp_dag.py
â”‚   â”œâ”€â”€ silver_to_gold_dag.py
â”‚   â””â”€â”€ silver_to_gold_gcp_dag.py
â”œâ”€â”€ dags
â”‚   â””â”€â”€ cdp_orders_bronze_to_gold.py
â”œâ”€â”€ logs
â”œâ”€â”€ plugins
â””â”€â”€ requirements-airflow.txt
```

### 3.3 QUI Æ¯á»šC Äáº¶T TÃŠN FILE DAG:

ğŸ“„ File name = business flow

ğŸ†” dag_id = business flow

TÃªn Ä‘Ãºng:
```text
cdp_orders_bronze_to_gold.py
```
```python
dag_id="cdp_orders_bronze_to_gold"
```
#### ğŸ“ QUY Æ¯á»šC CHUNG

ğŸ§© File name
```code
<domain>_<subject>_<layer_flow>.py
```

**<ins>VÃ­ dá»¥</ins>:**

|Use case|File|
|--------|----|
|Orders CDP|cdp_orders_bronze_to_gold.py|
|Transactions fraud|cdp_txn_bronze_to_gold.py|
|Customer 360|cdp_customer_bronze_to_gold.py|


### 3.4 Äá»”I DAG ID THEO CHUáº¨N:

```text
cdp_orders_bronze_to_gold
```
**Diá»…n giáº£i:**
* cdp = domain
* orders = subject
* bronze_to_gold = business flow

#### ğŸ†” DAG ID

```code
<domain>_<subject>_<layer_flow>
```

> ğŸ‘‰ file name == dag_id (best practice)

#### ğŸ”„ INFRA Äá»‚ á» ÄÃ‚U?

> ğŸ‘‰ Trong code task, khÃ´ng trong tÃªn:

```python
DataprocCreateBatchOperator(
    task_id="spark_bronze_to_silver",
    ...
)
```

### 3.5 âœ… QUY Æ¯á»šC Äáº¶T TÃŠN FILE SPARK JOB:

> **NguyÃªn táº¯c cá»‘t lÃµi**
> * ğŸ‘‰ TÃªn Spark job = mÃ´ táº£ nghiá»‡p vá»¥ + táº§ng dá»¯ liá»‡u
> * ğŸ‘‰ KHÃ”NG gáº¯n háº¡ táº§ng (dataproc / serverless / k8s / spark-submit)

#### ğŸ“ CÃ´ng thá»©c chuáº©n

```text
<domain>_<subject>_<layer>_job.py
```

#### ğŸ“Š Ãp dá»¥ng cho CDP demo "Orders" cho demo nÃ y:

Bronze â†’ Silver
```text
cdp_orders_bronze_job.py
```

Silver â†’ Gold
```text
cdp_orders_gold_job.py
```

> ğŸ‘‰ layer Ä‘Ã­ch lÃ  Ä‘á»§

<ins>**Giáº£i thÃ­ch:**</ins>

|LÃ½ do|Giáº£i thÃ­ch|
|-----|----------|
|Business-first|Reviewer chá»‰ cáº§n tÃªn file|
|KhÃ´ng phá»¥ thuá»™c háº¡ táº§ng|Spark cháº¡y á»Ÿ Ä‘Ã¢u cÅ©ng Ä‘Æ°á»£c|
|Dá»… má»Ÿ rá»™ng|Sau nÃ y thÃªm cdp_orders_risk_gold_job.py|
|Chuáº©n audit|Bank & Big4 ráº¥t thÃ­ch|

#### ğŸ“ Cáº¤U TRÃšC THÆ¯ Má»¤C SPARK JOB:

```text
jobs/
â”œâ”€â”€ cdp/
â”‚   â””â”€â”€ orders/
â”‚       â”œâ”€â”€ cdp_orders_bronze_job.py
â”‚       â”œâ”€â”€ cdp_orders_gold_job.py
â”‚       â””â”€â”€ schemas.py
```

ğŸ‘‰ Sau nÃ y:

```text
jobs/cdp/customer/
jobs/cdp/transactions/
```

#### ğŸ§© BÃŠN TRONG FILE â€“ QUY Æ¯á»šC Báº®T BUá»˜C:

##### 1ï¸âƒ£ main() luÃ´n tá»“n táº¡i:

```python

if __name__ == "__main__":
...
```

hoáº·c 

```python
def main(...):
    ...
```

##### 2ï¸âƒ£ App name = giá»‘ng tÃªn file:

```python
SparkSession.builder.appName("cdp-orders-bronze")
```

> âœ” Khi xem Dataproc / Spark UI â†’ ráº¥t rÃµ

##### 3ï¸âƒ£ KhÃ´ng hardcode ENV:

**âŒ Sai:**

```python
"gs://cdp-dem-silver/orders"
```

**âœ… ÄÃºng:**

```python
sys.argv[1]
```

##### 4ï¸âƒ£ Má»™t job = má»™t trÃ¡ch nhiá»‡m

**âœ” Bronze job:**
* 	cast
* 	cast
* 	cast
* 	clean
* 	partition
*	audit column

**âœ” Gold job:**
*	business shape
*	KPI-ready
*	push BigQuery

> ğŸ‘‰ KhÃ´ng trá»™n logic

##### ğŸ” Mapping DAG â†” Spark job 

|DAG|Spark job|
|---|---------|
|cdp_orders_bronze_to_gold|cdp_orders_bronze_job.py â†’ cdp_orders_gold_job.py|

> * DAG = orchestration
> * Spark = compute

### 3.6 CHUáº¨N HOÃ GCP:

#### ğŸ¯ Má»¥c tiÃªu
*	KhÃ´ng hardcode path
* 	Dá»… deploy Airflow
*	Chuáº©n enterprise (Composer / CI-CD)

Äáº¿n bÆ°á»›c nÃ y, hiá»‡n táº¡i GCP Ä‘ang cÃ³:

```bash
gs://cdp-dem-bronze/   # raw data + jobs
gs://cdp-dem-silver/   # curated parquet
gs://cdp-dem-gold/     # (optional) curated outputs
gs://cdp-dem-code/     # â¬…ï¸ bucket nÃ y ráº¥t quan trá»ng
gs://cdp-dem-bq-temp/  # BigQuery temp
```

> ğŸ‘‰ Cáº§n cÃ³ thay Ä‘á»•i nhá»

#### Chuáº©n hoÃ¡:

Hiá»‡n táº¡i báº¡n Ä‘ang Ä‘á»ƒ Spark job á»Ÿ:

```bash
gs://cdp-dem-bronze/jobs/
```

> âŒ khÃ´ng chuáº©n enterprise

##### âœ… Chuáº©n GCP Bucket:

|Loáº¡i|Bucket|Má»¥c Ä‘Ã­ch|
|----|------|--------|
|data|cdp-dem-bronze|raw data|
|code|cdp-dem-code|spark jobs|
|temp|cdp-dem-bq-temp|BigQuery|

##### ğŸ‘‰ Thá»±c hiá»‡n:

```bash
# táº¡o folder logic (GCS khÃ´ng cáº§n mkdir tháº­t)
gsutil cp -r ./jobs/cdp gs://cdp-dem-code/jobs/
```

Kiá»ƒm tra:

```bash
gsutil ls gs://cdp-dem-code/jobs/cdp/orders/
```

Ta sáº½ cÃ³:

```code
gs://cdp-dem-code/jobs/cdp/orders/cdp_orders_bronze_job.py
gs://cdp-dem-code/jobs/cdp/orders/cdp_orders_gold_job.py
```

âœ” KhÃ´ng xoÃ¡ jobs á»Ÿ bronze vá»™i
âœ” Chuyá»ƒn dáº§n sang cdp-dem-code

##### Chuáº©n hoÃ¡ cÃ¡ch gá»i Spark job (SERVERLESS / AIRFLOW READY):

**VÃ­ dá»¥ chuáº©n (Dataproc Serverless)**
```bash
gcloud dataproc batches submit pyspark \
  gs://cdp-dem-code/jobs/cdp/orders/cdp_orders_bronze_job.py \
  --region asia-southeast1 \
  -- \
  gs://cdp-dem-bronze/orders \
  gs://cdp-dem-silver/orders
```
> âœ” Khi sang Airflow â†’ chá»‰ copy

##### Chuáº©n hoÃ¡ Naming ENV:

|ThÃ nh pháº§n|Chuáº©n|
|----------|-----|
|Project|cdp-dem-project|
|Region|asia-southeast1|
|Dataset|cdp_gold|
|Domain|orders|



### 3.7 LOCK VERSION AIRFLOW:

Trong requirements.txt:
```text
apache-airflow==2.10.5
apache-airflow-providers-google==10.20.0
```
ğŸ‘‰ KhÃ´ng dÃ¹ng latest trong enterprise