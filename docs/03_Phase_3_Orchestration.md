# Phase 3: Airflow orchestration

## Overview:
1.	Refactor job â†’ idempotent
2.	Airflow DAG trigger Dataproc Serverless
3.	Data dependency (Bronze â†’ Silver â†’ Gold)
4.	Retry / alert / logging
5.	Dev (Codespace) â†’ Prod (Composer)

## ğŸ¯ Step 1 â€“ Chuáº©n hoÃ¡ job cho Airflow
Airflow cháº¡y theo schedule â†’ cáº§n incremental

### âœ… 1.1: CHá»T NGUYÃŠN Táº®C ENTERPRISE

#### Silver:
*	Partition by order_date
*	CÃ³ thá»ƒ overwrite theo partition

#### Gold:
*	KhÃ´ng overwrite toÃ n báº£ng
*	Load theo ngÃ y (partition)

### 1.2 âœï¸ Sá»¬A silver_to_gold.py:

#### Ná»™i dung edit
```python
def write_gold(df, project, dataset, table):
    (
        df.write
        .format("bigquery")
        .option("table", f"{project}:{dataset}.{table}")
        .option("temporaryGcsBucket", "cdp-dem-bq-temp")
        .option("partitionField", "order_date")   # ğŸ‘ˆ QUAN TRá»ŒNG
        .option("partitionType", "DAY")
        .mode("append")                            # ğŸ‘ˆ KHÃ”NG overwrite
        .save()
    )
```

#### full file:
```python
import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date


def get_spark(app_name: str) -> SparkSession:
    return (
        SparkSession.builder
        .appName(app_name)
        # BigQuery connector cÃ³ sáºµn trÃªn Dataproc
        .getOrCreate()
    )


def read_silver(spark: SparkSession, path: str):
    return spark.read.parquet(path)


def transform_to_gold(df):
    return (
        df
        .withColumn("order_date", to_date(col("order_ts")))
        .select(
            "order_id",
            "customer_id",
            "amount",
            "currency",
            "order_ts",
            "order_date"
        )
    )

def write_gold(df, project, dataset, table):
    (
        df.write
        .format("bigquery")
        .option("table", f"{project}:{dataset}.{table}")
        .option("temporaryGcsBucket", "cdp-dem-bq-temp")
        .option("partitionField", "order_date")   
        .option("partitionType", "DAY")
        .mode("append")                            
        .save()
    )

def main(silver_path, project, dataset, table):
    spark = get_spark("silver-to-gold")

    df_silver = read_silver(spark, silver_path)
    df_gold = transform_to_gold(df_silver)

    write_gold(df_gold, project, dataset, table)

    # test
    # df_gold.show()
    # df_gold.printSchema()

    spark.stop()


if __name__ == "__main__":
    if len(sys.argv) != 5:
        print(
            "Usage: spark-submit silver_to_gold.py "
            "<silver_path> <project> <dataset> <table>"
        )
        sys.exit(1)

    silver_path = sys.argv[1]
    project = sys.argv[2]
    dataset = sys.argv[3]
    table = sys.argv[4]

    main(silver_path, project, dataset, table)
```

### 1.3 Test code:

1ï¸âƒ£ Upload láº¡i job
```bash
gsutil cp jobs/silver_to_gold.py gs://cdp-dem-bronze/jobs/
```
**Cáº§n remove table cÅ©, do table trÆ°á»›c Ä‘Ã³ ko cÃ³ partition:**
```bash
bq rm -f -t cdp-dem-project:cdp_gold.orders
```
2ï¸âƒ£ Cháº¡y láº¡i batch:
```bash
gcloud dataproc batches submit pyspark \
  gs://cdp-dem-bronze/jobs/silver_to_gold.py \
  --region asia-southeast1 \
  -- \
  gs://cdp-dem-silver/orders \
  cdp-dem-project \
  cdp_gold \
  orders
```
3ï¸âƒ£ Check BigQuery
```bash
bq show cdp-dem-project:cdp_gold.orders
```

## Step 2: New DAG: 

**Má»¥c tiÃªu:***
Codespace chá»‰ Ä‘á»ƒ DEV
Airflow chá»‰ Ä‘á»ƒ ORCHESTRATE
Spark cháº¡y trÃªn Dataproc Serverless

**Kiáº¿n trÃºc**
```text
Airflow DAG
   |
   |-- bronze_to_silver (Dataproc Serverless)
   |
   |-- silver_to_gold   (Dataproc Serverless)
```
**ThÃ nh pháº§n:**
|ThÃ nh pháº§n|Vai trÃ²|
|----------|-------|
|Codespace|Dev DAG|
|Airflow local|Validate DAG|
|Dataproc Serverless|Run Spark|
|BigQuery|Serving|
|Composer|Prod (sau)|


### 2.1 ğŸ“ dags/bronze_to_gold_dag.py:

```python
from airflow import DAG
from airflow.providers.google.cloud.operators.dataproc import (
    DataprocCreateBatchOperator
)
from airflow.models import Variable
from datetime import datetime

PROJECT_ID = Variable.get("PROJECT_ID")
REGION = Variable.get("REGION")

BRONZE_PATH = Variable.get("BRONZE_PATH")
SILVER_PATH = Variable.get("SILVER_PATH")

BQ_DATASET = Variable.get("BQ_DATASET")
BQ_TABLE = Variable.get("BQ_TABLE")

JOB_BUCKET = Variable.get("JOB_BUCKET")

with DAG(
    dag_id="bronze_to_gold_dataproc_serverless",
    start_date=datetime(2025, 12, 12),
    schedule=None,
    catchup=False,
    tags=["cdp", "dataproc", "serverless"],
) as dag:

    bronze_to_silver = DataprocCreateBatchOperator(
        task_id="bronze_to_silver",
        project_id=PROJECT_ID,
        region=REGION,
        batch={
            "pysparkBatch": {
                "mainPythonFileUri": f"gs://{JOB_BUCKET}/jobs/bronze_to_silver.py",
                "args": [BRONZE_PATH, SILVER_PATH],
            }
        },
    )

    silver_to_gold = DataprocCreateBatchOperator(
        task_id="silver_to_gold",
        project_id=PROJECT_ID,
        region=REGION,
        batch={
            "pysparkBatch": {
                "mainPythonFileUri": f"gs://{JOB_BUCKET}/jobs/silver_to_gold.py",
                "args": [
                    SILVER_PATH,
                    PROJECT_ID,
                    BQ_DATASET,
                    BQ_TABLE,
                ],
            }
        },
    )

    bronze_to_silver >> silver_to_gold
```

### 2.2 TEST LOCAL (CODESPACE):

```bash
airflow dags list | grep bronze_to_gold
airflow dags test bronze_to_gold_dataproc_serverless 2025-12-12
```
**ğŸ‘‰ Airflow sáº½:**
*	KhÃ´ng cháº¡y Spark local
*	Chá»‰ submit Dataproc Serverless batch

## Step 3: Chuáº©n hoÃ¡ Airflow Dev (Professional / Bank-grade):

**<ins>Má»¥c tiÃªu:</ins>**
*	KhÃ´ng Ä‘á»ƒ example DAG
*	KhÃ´ng láº«n DAG local / DAG prod
*	Chuáº©n bá»‹ CI/CD Ä‘áº©y lÃªn Cloud Composer

### 3.1 Táº®T EXAMPLE DAG:

Táº¯t:
```bash
export AIRFLOW__CORE__LOAD_EXAMPLES=False
```

Check:
```bash
airflow dags list | grep example
```

### 3.2 Cáº¤U TRÃšC DAG CHUáº¨N:

Backup dags dev and change name:
```bash
mkdir -p ./.airflow/backup_dags
mv ./.airflow/dags/* ./.airflow/backup_dags
cp ./.airflow/backup_dags/bronze_to_gold_dataproc_serverless.py ./.airflow/dags/
```
Cáº¥u trÃºc:
```text
((.venv) ) @nptan2005 âœ /workspaces/spark401-codespace (main) $ tree -L4 ./.airflow/
./.airflow/
â”œâ”€â”€ README.md
â”œâ”€â”€ airflow.cfg
â”œâ”€â”€ airflow.db
â”œâ”€â”€ backup_dags
â”‚   â”œâ”€â”€ bronze_to_gold_dataproc_serverless.py
â”‚   â”œâ”€â”€ bronze_to_silver_dag.py
â”‚   â”œâ”€â”€ bronze_to_silver_gcp_dag.py
â”‚   â”œâ”€â”€ silver_to_gold_dag.py
â”‚   â””â”€â”€ silver_to_gold_gcp_dag.py
â”œâ”€â”€ dags
â”‚   â””â”€â”€ cdp_orders_bronze_to_gold.py
â”œâ”€â”€ logs
â”œâ”€â”€ plugins
â””â”€â”€ requirements-airflow.txt
```

### 3.3 QUI Æ¯á»šC Äáº¶T TÃŠN FILE DAG:

ğŸ“„ File name = business flow

ğŸ†” dag_id = business flow

TÃªn Ä‘Ãºng:
```text
cdp_orders_bronze_to_gold.py
```
```python
dag_id="cdp_orders_bronze_to_gold"
```
#### ğŸ“ QUY Æ¯á»šC CHUNG

ğŸ§© File name
```code
<domain>_<subject>_<layer_flow>.py
```

**<ins>VÃ­ dá»¥</ins>:**

|Use case|File|
|--------|----|
|Orders CDP|cdp_orders_bronze_to_gold.py|
|Transactions fraud|cdp_txn_bronze_to_gold.py|
|Customer 360|cdp_customer_bronze_to_gold.py|


### 3.4 Äá»”I DAG ID THEO CHUáº¨N:

```text
cdp_orders_bronze_to_gold
```
**Diá»…n giáº£i:**
* cdp = domain
* orders = subject
* bronze_to_gold = business flow

#### ğŸ†” DAG ID

```code
<domain>_<subject>_<layer_flow>
```

> ğŸ‘‰ file name == dag_id (best practice)

#### ğŸ”„ INFRA Äá»‚ á» ÄÃ‚U?

> ğŸ‘‰ Trong code task, khÃ´ng trong tÃªn:

```python
DataprocCreateBatchOperator(
    task_id="spark_bronze_to_silver",
    ...
)
```

### 3.5 âœ… QUY Æ¯á»šC Äáº¶T TÃŠN FILE SPARK JOB:

> **NguyÃªn táº¯c cá»‘t lÃµi**
> * ğŸ‘‰ TÃªn Spark job = mÃ´ táº£ nghiá»‡p vá»¥ + táº§ng dá»¯ liá»‡u
> * ğŸ‘‰ KHÃ”NG gáº¯n háº¡ táº§ng (dataproc / serverless / k8s / spark-submit)

#### ğŸ“ CÃ´ng thá»©c chuáº©n

```text
<domain>_<subject>_<layer>_job.py
```

#### ğŸ“Š Ãp dá»¥ng cho CDP demo "Orders" cho demo nÃ y:

Bronze â†’ Silver
```text
cdp_orders_bronze_job.py
```

Silver â†’ Gold
```text
cdp_orders_gold_job.py
```

> ğŸ‘‰ layer Ä‘Ã­ch lÃ  Ä‘á»§

<ins>**Giáº£i thÃ­ch:**</ins>

|LÃ½ do|Giáº£i thÃ­ch|
|-----|----------|
|Business-first|Reviewer chá»‰ cáº§n tÃªn file|
|KhÃ´ng phá»¥ thuá»™c háº¡ táº§ng|Spark cháº¡y á»Ÿ Ä‘Ã¢u cÅ©ng Ä‘Æ°á»£c|
|Dá»… má»Ÿ rá»™ng|Sau nÃ y thÃªm cdp_orders_risk_gold_job.py|
|Chuáº©n audit|Bank & Big4 ráº¥t thÃ­ch|

#### ğŸ“ Cáº¤U TRÃšC THÆ¯ Má»¤C SPARK JOB:

```text
jobs/
â”œâ”€â”€ cdp/
â”‚   â””â”€â”€ orders/
â”‚       â”œâ”€â”€ cdp_orders_bronze_job.py
â”‚       â”œâ”€â”€ cdp_orders_gold_job.py
â”‚       â””â”€â”€ schemas.py
```

ğŸ‘‰ Sau nÃ y:

```text
jobs/cdp/customer/
jobs/cdp/transactions/
```

#### ğŸ§© BÃŠN TRONG FILE â€“ QUY Æ¯á»šC Báº®T BUá»˜C:

##### 1ï¸âƒ£ main() luÃ´n tá»“n táº¡i:

```python

if __name__ == "__main__":
...
```

hoáº·c 

```python
def main(...):
    ...
```

##### 2ï¸âƒ£ App name = giá»‘ng tÃªn file:

```python
SparkSession.builder.appName("cdp-orders-bronze")
```

> âœ” Khi xem Dataproc / Spark UI â†’ ráº¥t rÃµ

##### 3ï¸âƒ£ KhÃ´ng hardcode ENV:

**âŒ Sai:**

```python
"gs://cdp-dem-silver/orders"
```

**âœ… ÄÃºng:**

```python
sys.argv[1]
```

##### 4ï¸âƒ£ Má»™t job = má»™t trÃ¡ch nhiá»‡m

**âœ” Bronze job:**
* 	cast
* 	cast
* 	cast
* 	clean
* 	partition
*	audit column

**âœ” Gold job:**
*	business shape
*	KPI-ready
*	push BigQuery

> ğŸ‘‰ KhÃ´ng trá»™n logic

##### ğŸ” Mapping DAG â†” Spark job 

|DAG|Spark job|
|---|---------|
|cdp_orders_bronze_to_gold|cdp_orders_bronze_job.py â†’ cdp_orders_gold_job.py|

> * DAG = orchestration
> * Spark = compute

### 3.6 CHUáº¨N HOÃ GCP:

#### ğŸ¯ Má»¥c tiÃªu
*	KhÃ´ng hardcode path
* 	Dá»… deploy Airflow
*	Chuáº©n enterprise (Composer / CI-CD)

Äáº¿n bÆ°á»›c nÃ y, hiá»‡n táº¡i GCP Ä‘ang cÃ³:

```bash
gs://cdp-dem-bronze/   # raw data + jobs
gs://cdp-dem-silver/   # curated parquet
gs://cdp-dem-gold/     # (optional) curated outputs
gs://cdp-dem-code/     # â¬…ï¸ bucket nÃ y ráº¥t quan trá»ng
gs://cdp-dem-bq-temp/  # BigQuery temp
```

> ğŸ‘‰ Cáº§n cÃ³ thay Ä‘á»•i nhá»

#### Chuáº©n hoÃ¡:

Hiá»‡n táº¡i báº¡n Ä‘ang Ä‘á»ƒ Spark job á»Ÿ:

```bash
gs://cdp-dem-bronze/jobs/
```

> âŒ khÃ´ng chuáº©n enterprise

##### âœ… Chuáº©n GCP Bucket:

|Loáº¡i|Bucket|Má»¥c Ä‘Ã­ch|
|----|------|--------|
|data|cdp-dem-bronze|raw data|
|code|cdp-dem-code|spark jobs|
|temp|cdp-dem-bq-temp|BigQuery|

##### ğŸ‘‰ Thá»±c hiá»‡n:

```bash
# táº¡o folder logic (GCS khÃ´ng cáº§n mkdir tháº­t)
gsutil cp -r ./jobs/cdp gs://cdp-dem-code/jobs/
```

Kiá»ƒm tra:

```bash
gsutil ls gs://cdp-dem-code/jobs/cdp/orders/
```

Ta sáº½ cÃ³:

```code
gs://cdp-dem-code/jobs/cdp/orders/cdp_orders_bronze_job.py
gs://cdp-dem-code/jobs/cdp/orders/cdp_orders_gold_job.py
```

âœ” KhÃ´ng xoÃ¡ jobs á»Ÿ bronze vá»™i
âœ” Chuyá»ƒn dáº§n sang cdp-dem-code

##### Chuáº©n hoÃ¡ cÃ¡ch gá»i Spark job (SERVERLESS / AIRFLOW READY):

**VÃ­ dá»¥ chuáº©n (Dataproc Serverless)**
```bash
gcloud dataproc batches submit pyspark \
  gs://cdp-dem-code/jobs/cdp/orders/cdp_orders_bronze_job.py \
  --region asia-southeast1 \
  -- \
  gs://cdp-dem-bronze/orders \
  gs://cdp-dem-silver/orders
```
> âœ” Khi sang Airflow â†’ chá»‰ copy

##### Chuáº©n hoÃ¡ Naming ENV:

|ThÃ nh pháº§n|Chuáº©n|
|----------|-----|
|Project|cdp-dem-project|
|Region|asia-southeast1|
|Dataset|cdp_gold|
|Domain|orders|



### 3.7 LOCK VERSION AIRFLOW:

Trong requirements.txt:
```text
apache-airflow==2.10.5
apache-airflow-providers-google==10.20.0
```
ğŸ‘‰ KhÃ´ng dÃ¹ng latest trong enterprise

## Step 4: Triá»ƒn khai Cloud Composer 2 (Airflow enterprise)

> Codespace = code & CI
> GCP = runtime & orchestration

### ğŸ¯ Má»¥c tiÃªu:

* CÃ³ Airflow tháº­t cháº¡y trÃªn GCP (Composer 2)
* DAG gá»i Dataproc Serverless (khÃ´ng giá»¯ cluster)
* KhÃ´ng hardcode secret
* Chuáº©n ngÃ¢n hÃ ng / enterprise

### 4.1 Táº O CLOUD COMPOSER 2 (CHUáº¨N + TIáº¾T KIá»†M):

#### âœ… Chá»n version á»•n Ä‘á»‹nh:

|ThÃ nh pháº§n|Version|
|----------|-------|
|Composer|2.16.1|
|Airflow|2.10.5 âœ…|
|Python|3.10|

> ğŸ‘‰ 2.10.5 lÃ  báº£n LTS á»•n Ä‘á»‹nh nháº¥t hiá»‡n táº¡i cho enterprise

#### ğŸ”§ Lá»‡nh táº¡o Composer (báº£n tá»‘i Æ°u chi phÃ­):

##### 1ï¸âƒ£ Táº¡o Service Account

```bash
gcloud iam service-accounts create cdp-composer-sa \
  --display-name "CDP Composer Service Account" \
  --project cdp-dem-project
```

ğŸ‘‰ Service Account sáº½ cÃ³ dáº¡ng:

```code
cdp-composer-sa@cdp-dem-project.iam.gserviceaccount.com
```

##### 2ï¸âƒ£ GÃ¡n quyá»n Báº®T BUá»˜C
##### GÃN IAM CHUáº¨N (ENTERPRISE MINIMAL)

```bash
PROJECT_ID=cdp-dem-project
COMPOSER_SA=cdp-composer-sa@$PROJECT_ID.iam.gserviceaccount.com
```

1. Composer worker:

```bash
gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member="serviceAccount:$COMPOSER_SA" \
  --role="roles/composer.worker"
```

2. Dataproc Serverless:

```bash
gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member="serviceAccount:$COMPOSER_SA" \
  --role="roles/dataproc.editor"
```

3. BigQuery:

```bash
gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member="serviceAccount:$COMPOSER_SA" \
  --role="roles/bigquery.jobUser"

gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member="serviceAccount:$COMPOSER_SA" \
  --role="roles/bigquery.dataEditor"
```
4. GCS (bronze / silver / gold / temp):

```bash
gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member="serviceAccount:$COMPOSER_SA" \
  --role="roles/storage.objectAdmin"
```

5. GÃ¡n role Composer ServiceAgentV2Ext:

XÃ¡c Ä‘á»‹nh Composer Service Agent

```code
service-585752501826@cloudcomposer-accounts.iam.gserviceaccount.com
```

Grant

```bash
PROJECT_ID=cdp-dem-project
COMPOSER_AGENT=service-585752501826@cloudcomposer-accounts.iam.gserviceaccount.com

gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member="serviceAccount:$COMPOSER_AGENT" \
  --role="roles/composer.ServiceAgentV2Ext"
```
hoáº·c

```bash
PROJECT_ID=cdp-dem-project

gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member="serviceAccount:service-585752501826@cloudcomposer-accounts.iam.gserviceaccount.com" \
  --role="roles/composer.ServiceAgentV2Ext"
```

> ğŸ“Œ ÄÃ¢y lÃ  set quyá»n chuáº©n CDP enterprise (khÃ´ng dÆ°)

#### Táº O COMPOSER

> âš ï¸ Cháº¡y má»™t láº§n, máº¥t ~20â€“30 phÃºt

```bash
gcloud composer environments create cdp-airflow \
  --location asia-southeast1 \
  --image-version composer-3-airflow-2.10.5 \
  --environment-size small \
  --service-account cdp-composer-sa@cdp-dem-project.iam.gserviceaccount.com \
  --project cdp-dem-project
```


##### ğŸ“Œ Giáº£i thÃ­ch nhanh:
*	environment-size small â†’ tiáº¿t kiá»‡m ~$250/thÃ¡ng
*	KhÃ´ng báº­t K8s workload dÆ°
*	KhÃ´ng custom image (chÆ°a cáº§n)

##### Check

```bash
gcloud composer environments describe cdp-airflow \
  --location asia-southeast1
```

##### Check status

```bash
gcloud composer environments describe cdp-airflow \
  --location asia-southeast1 \
  --project cdp-dem-project \
  --format="value(state)"
```

##### Check log

```bash
gcloud composer operations list \
  --locations asia-southeast1 \
  --project cdp-dem-project
```

```bash
gcloud composer operations describe <OPERATION_ID> \
  --location asia-southeast1
```

Ex:

```bash
gcloud composer operations describe 7e86687f-6839-41fe-83be-ffe3da51d751 \
  --location asia-southeast1
```

#### Delete Composer

```bash
gcloud composer environments delete cdp-airflow \
  --location asia-southeast1 \
  --project cdp-dem-project
```

##### ÄÃ¡nh gÃ­a tiÃªu chuáº©n:

|ThÃ nh pháº§n|Chuáº©n enterprise|
|----------|----------------|
|Composer SA riÃªng|âœ…|
|KhÃ´ng dÃ¹ng default SA|âœ…|
|TÃ¡ch quyá»n Dataproc|âœ…|
|TÃ¡ch quyá»n BigQuery|âœ…|
|Composer ServiceAgentV2Ext|âœ… (cÃ¡i nÃ y nhiá»u ngÆ°á»i thiáº¿u)|

## Step 5: Airflow Enterprise trÃªn GCP (Composer 3 + Dataproc Serverless)

#### ğŸ¯ Má»¥c tiÃªu:

* Codespace chá»‰ Ä‘á»ƒ code & CI/CD
* Airflow cháº¡y 100% trÃªn GCP
* Spark cháº¡y báº±ng Dataproc Serverless
* Cáº¥u trÃºc Ä‘Ãºng chuáº©n Enterprise / Banking

### 5.1 â€“ Láº¤Y DAG BUCKET Cá»¦A COMPOSER

```bash
gcloud composer environments describe cdp-airflow \
  --location asia-southeast1 \
  --project cdp-dem-project \
  --format="value(config.dagGcsPrefix)"
```

ğŸ“ŒKáº¿t quáº£:

```code
gs://asia-southeast1-cdp-airflow-e00866e0-bucket/dags
```

ğŸ‘‰ÄÃ¢y lÃ :
```code
COMPOSER_DAG_BUCKET
```

### 5.2 QUY Æ¯á»šC CHUáº¨N ENTERPRISE:

#### ğŸ“‚ Local (Codespace):

```code
.airflow/
â””â”€â”€ dags/
    â””â”€â”€ cdp/
        â””â”€â”€ orders/
            â””â”€â”€ cdp_orders_bronze_to_gold.py
```

>ğŸ‘‰ **Má»—i domain = 1 folder**
>* cdp/orders
>* cdp/customers
>* cdp/transactions

### 5.3 UPLOAD DAG LÃŠN COMPOSER

```bash
gsutil rsync -r .airflow/dags/cdp \
  gs://asia-southeast1-cdp-airflow-e00866e0-bucket/dags/cdp
```

>ğŸ“Œ **LÆ°u Ã½:**
>* KHÃ”NG cáº§n restart
>* Airflow auto-detect sau 30â€“60s

### 5.4 KIá»‚M TRA AIRFLOW UI:

```bash
gcloud composer environments describe cdp-airflow \
  --location asia-southeast1 \
  --project cdp-dem-project \
  --format="value(config.airflowUri)"
```

> â¡ï¸ Má»Ÿ link â†’ Ä‘Äƒng nháº­p Google

sáº½ tháº¥y DAG:
```code
cdp_orders_bronze_to_gold
```
### 5.5 DAG CHUáº¨N ENTERPRISE (Dataproc Serverless)

```python
from airflow import DAG
from airflow.providers.google.cloud.operators.dataproc import DataprocCreateBatchOperator
from airflow.models import Variable
from datetime import datetime

PROJECT_ID = Variable.get("PROJECT_ID")
REGION = Variable.get("REGION")

JOB_BUCKET = Variable.get("JOB_BUCKET")
BRONZE_PATH = Variable.get("BRONZE_PATH")
SILVER_PATH = Variable.get("SILVER_PATH")
BQ_DATASET = Variable.get("BQ_DATASET")
BQ_TABLE = Variable.get("BQ_TABLE")

with DAG(
    dag_id="cdp_orders_bronze_to_gold",
    start_date=datetime(2024, 1, 1),
    schedule=None,
    catchup=False,
    tags=["cdp", "orders", "dataproc", "serverless"],
) as dag:

    bronze_to_silver = DataprocCreateBatchOperator(
        task_id="bronze_to_silver",
        project_id=PROJECT_ID,
        region=REGION,
        batch={
            "pyspark_batch": {
                "main_python_file_uri": f"gs://{JOB_BUCKET}/jobs/cdp/orders/cdp_orders_bronze_job.py",
                "args": [
                    BRONZE_PATH,
                    SILVER_PATH,
                ],
            }
        },
    )

    silver_to_gold = DataprocCreateBatchOperator(
        task_id="silver_to_gold",
        project_id=PROJECT_ID,
        region=REGION,
        batch={
            "pyspark_batch": {
                "main_python_file_uri": f"gs://{JOB_BUCKET}/jobs/cdp/orders/cdp_orders_gold_job.py",
                "args": [
                    SILVER_PATH,
                    PROJECT_ID,
                    BQ_DATASET,
                    BQ_TABLE,
                ],
            }
        },
    )

    bronze_to_silver >> silver_to_gold
```

#### ğŸ“Œ LÆ¯U Ã KHÃC BIá»†T LOCAL TEST VS CLOUD (SERVERLESS VS CLUSTER)

|ThÃ nh pháº§n|Cluster|Serverless|
|----------|-------|----------|
|Operator|DataprocSubmitJobOperator âŒ|DataprocCreateBatchOperator âœ…|
|placement.cluster_name|Báº¯t buá»™c|âŒ KhÃ´ng dÃ¹ng|
|Batch|âŒ|âœ…|
|Pay-per-job|âŒ|âœ…|
|Enterprise|âš ï¸|âœ…|


### 5.6 AIRFLOW VARIABLES (TRÃŠN COMPOSER):

> **Airflow UI â†’ Admin â†’ Variables:**

|Key|Value|
|---|-----|
|PROJECT_ID|cdp-dem-project|
|REGION|asia-southeast1|
|JOB_BUCKET|cdp-dem-code|
|BRONZE_PATH|gs://cdp-dem-bronze/orders|
|SILVER_PATH|gs://cdp-dem-silver/orders|
|BQ_DATASET|cdp_gold|
|BQ_TABLE|orders|

### 5.7 COPY JOB 

```bash
gsutil cp jobs/cdp/orders/cdp_orders_bronze_job.py gs://cdp-dem-bronze/jobs/cdp/orders

gsutil cp jobs/cdp/orders/cdp_orders_gold_job.py gs://cdp-dem-bronze/jobs/cdp/orders
```

### 5.8 RUN DAG ğŸ‰:

>â¡ï¸ **Trigger DAG**
>â¡ï¸ **Theo dÃµi:**
>* Dataproc â†’ Batches
>* BigQuery â†’ table cdp_gold.orders

### 5.9 fix lá»—i (1):

#### IAM / GCP security:

##### âŒ Lá»—i:

```code
User not authorized to act as service account
'585752501826-compute@developer.gserviceaccount.com'
```

**ğŸ‘‰ Ã nghia**

>Cloud Composer (Airflow) Ä‘ang muá»‘n impersonate service account
>585752501826-compute@developer.gserviceaccount.com
>nhÆ°ng KHÃ”NG Ä‘Æ°á»£c phÃ©p.

#### ğŸ” Táº I SAO AIRFLOW Láº I DÃ™NG COMPUTE SA?

**Máº·c Ä‘á»‹nh:**
* Dataproc Serverless náº¿u khÃ´ng chá»‰ Ä‘á»‹nh execution_config.service_account
* ğŸ‘‰ nÃ³ fallback vá» Compute Engine default SA

```code
<PROJECT_NUMBER>-compute@developer.gserviceaccount.com
```
> ğŸ‘‰ VÃ  Composer worker SA khÃ´ng cÃ³ quyá»n â€œactAsâ€ SA nÃ y

#### âœ… CÃCH ÄÃšNG â€“ ENTERPRISE FIX (Báº®T BUá»˜C)

CÃ³ 2 cÃ¡ch, nhÆ°ng CHá»ˆ CÃCH 2 LÃ€ ÄÃšNG CHUáº¨N BANK/ENTERPRISE.

##### ğŸš« CÃCH 1 (Táº M)

Cho Composer actAs Compute SA

```bash
PROJECT_ID=cdp-dem-project
PROJECT_NUMBER=585752501826

gcloud iam service-accounts add-iam-policy-binding \
  ${PROJECT_NUMBER}-compute@developer.gserviceaccount.com \
  --member="serviceAccount:cdp-composer-sa@${PROJECT_ID}.iam.gserviceaccount.com" \
  --role="roles/iam.serviceAccountUser"
```

**âš ï¸ KhÃ´ng khuyáº¿n nghá»‹ vÃ¬:**
>*	Compute SA = quyá»n ráº¥t rá»™ng
>*	KhÃ´ng audit tá»‘t
>*	KhÃ´ng Ä‘áº¡t chuáº©n bank

##### âœ… CÃCH 2 (CHUáº¨N ENTERPRISE â€“ Báº®T BUá»˜C DÃ™NG)

###### ğŸ¯ Táº O SERVICE ACCOUNT RIÃŠNG CHO DATAPROC SERVERLESS

```bash
gcloud iam service-accounts create cdp-dataproc-sa \
  --display-name "CDP Dataproc Serverless SA" \
  --project cdp-dem-project
```

```code
DATAPROC_SA=cdp-dataproc-sa@cdp-dem-project.iam.gserviceaccount.com
```

###### ğŸ” GÃN ROLE CHUáº¨N

```bash
DATAPROC_SA=cdp-dataproc-sa@cdp-dem-project.iam.gserviceaccount.com

gcloud projects add-iam-policy-binding cdp-dem-project \
  --member="serviceAccount:$DATAPROC_SA" \
  --role="roles/dataproc.worker"

gcloud projects add-iam-policy-binding cdp-dem-project \
  --member="serviceAccount:$DATAPROC_SA" \
  --role="roles/storage.objectAdmin"

gcloud projects add-iam-policy-binding cdp-dem-project \
  --member="serviceAccount:$DATAPROC_SA" \
  --role="roles/bigquery.dataEditor"

gcloud projects add-iam-policy-binding cdp-dem-project \
  --member="serviceAccount:$DATAPROC_SA" \
  --role="roles/bigquery.jobUser"
```

###### ğŸ”‘ CHO COMPOSER ÄÆ¯á»¢C IMPERSONATE SA NÃ€Y

```bash
gcloud iam service-accounts add-iam-policy-binding \
  $DATAPROC_SA \
  --member="serviceAccount:cdp-composer-sa@cdp-dem-project.iam.gserviceaccount.com" \
  --role="roles/iam.serviceAccountUser"
```
> ğŸ‘‰ ÄÃ‚Y LÃ€ DÃ’NG QUYáº¾T Äá»ŠNH

###### ğŸ› ï¸ Sá»¬A DAG (Báº®T BUá»˜C)

Trong DataprocCreateBatchOperator PHáº¢I CHá»ˆ RÃ• service_account

```python
DataprocCreateBatchOperator(
    task_id="bronze_to_silver",
    project_id=PROJECT_ID,
    region=REGION,
    batch={
        "pyspark_batch": {
            "main_python_file_uri": "...",
            "args": [...],
        },
        "environment_config": {
            "execution_config": {
                "service_account": "cdp-dataproc-sa@cdp-dem-project.iam.gserviceaccount.com"
            }
        }
    },
)
```

##### Runtime config:

```python
"runtime_config": {
    "properties": {
        # Driver
        "spark.driver.cores": "4",
        "spark.driver.memory": "4g",

        # Executor
        "spark.executor.cores": "4",
        "spark.executor.memory": "4g",
        "spark.executor.instances": "2",

        # Optional â€“ giáº£m overhead
        "spark.sql.shuffle.partitions": "8",
    }
}
```

|Tham sá»‘|GiÃ¡ trá»‹ há»£p lá»‡|
|-------|--------------|
|spark.driver.cores|4 / 8 / 16|
|spark.executor.cores|4 / 8 / 16|
|spark.executor.instances|>= 2|
|spark.driver.memory|â‰¥ 4g (khuyáº¿n nghá»‹)|
|spark.executor.memory|â‰¥ 4g|


###### UPLOAD DAG LÃŠN COMPOSER

```python
from airflow import DAG
from airflow.providers.google.cloud.operators.dataproc import DataprocCreateBatchOperator
from airflow.models import Variable
from datetime import datetime

PROJECT_ID = Variable.get("PROJECT_ID")
REGION = Variable.get("REGION")

JOB_BUCKET = Variable.get("JOB_BUCKET")
BRONZE_PATH = Variable.get("BRONZE_PATH")
SILVER_PATH = Variable.get("SILVER_PATH")
BQ_DATASET = Variable.get("BQ_DATASET")
BQ_TABLE = Variable.get("BQ_TABLE")

with DAG(
    dag_id="cdp_orders_bronze_to_gold",
    start_date=datetime(2024, 1, 1),
    schedule=None,
    catchup=False,
    tags=["cdp", "orders", "dataproc", "serverless"],
) as dag:

    bronze_to_silver = DataprocCreateBatchOperator(
        task_id="bronze_to_silver",
        project_id=PROJECT_ID,
        region=REGION,
        batch={
            "pyspark_batch": {
                "main_python_file_uri": (
                    f"gs://{JOB_BUCKET}/jobs/cdp/orders/cdp_orders_bronze_job.py"
                ),
                "args": [
                    BRONZE_PATH,
                    SILVER_PATH,
                ],
            },

            # âœ… ÄÃšNG: runtime_config náº±m trong batch
            "runtime_config": {
                "properties": {
                    # Driver
                    "spark.driver.cores": "4",
                    "spark.driver.memory": "4g",

                    # Executor
                    "spark.executor.cores": "4",
                    "spark.executor.memory": "4g",
                    "spark.executor.instances": "2",

                    # Optional â€“ giáº£m overhead
                    "spark.sql.shuffle.partitions": "8",
                }
            },
            "environment_config": {
                "execution_config": {
                    "service_account": (
                        "cdp-dataproc-sa@cdp-dem-project.iam.gserviceaccount.com"
                    )
                }
            },
        },
    )

    silver_to_gold = DataprocCreateBatchOperator(
        task_id="silver_to_gold",
        project_id=PROJECT_ID,
        region=REGION,
        batch={
            "pyspark_batch": {
                "main_python_file_uri": f"gs://{JOB_BUCKET}/jobs/cdp/orders/cdp_orders_gold_job.py",
                "args": [
                    SILVER_PATH,
                    PROJECT_ID,
                    BQ_DATASET,
                    BQ_TABLE,
                ],
            },
             # âœ… ÄÃšNG: runtime_config náº±m trong batch
            "runtime_config": {
                "properties": {
                    # Driver
                    "spark.driver.cores": "4",
                    "spark.driver.memory": "4g",

                    # Executor
                    "spark.executor.cores": "4",
                    "spark.executor.memory": "4g",
                    "spark.executor.instances": "2",

                    # Optional â€“ giáº£m overhead
                    "spark.sql.shuffle.partitions": "8",
                }
            },
            "environment_config": {
                "execution_config": {
                    "service_account": (
                        "cdp-dataproc-sa@cdp-dem-project.iam.gserviceaccount.com"
                    )
                }
            },
        }
    )

    bronze_to_silver >> silver_to_gold
```

Copy

```bash
gsutil rsync -r .airflow/dags/cdp \
  gs://asia-southeast1-cdp-airflow-e00866e0-bucket/dags/cdp
```

### 5.10 fix lá»—i (2):

### bá»‹ CHáº¶N QUOTA GCP

#### âŒ NGUYÃŠN NHÃ‚N Lá»–I

Dataproc Serverless Spark tá»± scale tÃ i nguyÃªn Compute Engine phÃ­a sau.

Log bÃ¡o 3 lá»—i quota:

```code
1. Insufficient 'CPUS_ALL_REGIONS'
   Requested: 12.0
   Available: 0.0

2. Insufficient 'DISKS_TOTAL_GB'
   Requested: 1200 GB
   Available: 844 GB

3. CPU quota exceeded (min 2 workers required)
```

>ğŸ‘‰ TÃ i khoáº£n GCP Free / new project:
>*	CPU quota = 0
>*	Disk quota = < 1TB
>* Serverless Spark â†’ Ä‘Ã²i quota ráº¥t cao ngay tá»« Ä‘áº§u

* â›” KhÃ´ng cÃ³ cÃ¡ch â€œconfig nhá» hÆ¡n ná»¯aâ€ Ä‘á»ƒ nÃ© quota nÃ y
* â›” KhÃ´ng pháº£i bug

#### ğŸ¯ Má»¤C TIÃŠU HIá»†N Táº I

> â€œCháº¡y xong pipeline Bronze â†’ Silver â†’ Gold 1 láº§n thÃ nh cÃ´ng

**â†’ CÃ¡ch NGáº®N NHáº¤T â€“ ÃT Äá»¤NG NHáº¤T â€“ CHáº®C CHáº Y**

#### âœ… CÃCH FIX

> ğŸ‘‰ CHUYá»‚N Táº M THá»œI SANG DATAPROC CLUSTER (ON-DEMAND)

KHÃ”NG Ä‘á»•i Spark code
KHÃ”NG Ä‘á»•i DAG logic
CHá»ˆ Ä‘á»•i operator

##### NguyÃªn nhÃ¢n:

|Serverless|Dataproc Cluster|
|----------|----------------|
|Quota CPU|âŒ bá»‹ cháº·n|
|Disk quota|âŒ cao|
|Min cores|4â€“8|
|Má»¥c tiÃªu há»c/demo|âŒ|

##### So sÃ¡nh Dataproc Batch Job (Serverless) vs Dataproc Cluster

**ğŸ”¹ Dataproc Batch Job (Serverless)**

|TiÃªu chÃ­|Batch Job|
|--------|---------|
|CÃ¡ch cháº¡y||Má»—i job â†’ spin up Spark riÃªng|
|Quáº£n lÃ½ cluster|âŒ KhÃ´ng|
|Quota|âŒ Ráº¥t gáº¯t (CPU, Disk)|
|Free tier|âŒ Dá»… fail|
|Control Spark config|âŒ Bá»‹ giá»›i háº¡n|
|Airflow|DÃ¹ng DataprocCreateBatchOperator|
|PhÃ¹ há»£p|Job nhá», ad-hoc, prod cÃ³ quota lá»›n|

**ğŸ“Œ Thá»±c táº¿ báº¡n gáº·p hÃ´m nay**
>**â†’ Fail liÃªn tá»¥c vÃ¬:**
* CPU cores báº¯t buá»™c 4/8/16
* Min executor â‰¥ 2
* Quota CPU/DISK khÃ´ng Ä‘á»§

**â›” Káº¿t luáº­n:**
> ğŸ‘‰ KHÃ”NG phÃ¹ há»£p tÃ i khoáº£n GCP Free / há»c táº­p

**ğŸ”¹ Dataproc Cluster (Classic)**

|TiÃªu chÃ­|Cluster|
|--------|-------|
|CÃ¡ch cháº¡y|1 cluster â†’ nhiá»u job|
|Quáº£n lÃ½ cluster|âœ… CÃ³|
|Quota|âœ… Dá»… kiá»ƒm soÃ¡t|
|Free tier|âœ… Kháº£ thi|
|Control Spark config|âœ… Full|
|Airflow|DataprocSubmitJobOperator|
|PhÃ¹ há»£p|Enterprise, learning, CDP|

**ğŸ“Œ Giá»‘ng Hadoop / Spark on-prem**
**ğŸ“Œ ÄÃºng kiáº¿n trÃºc ngÃ¢n hÃ ng / CDP**

**âœ… Káº¿t luáº­n:**
> ğŸ‘‰ NÃŠN DÃ™NG CLUSTER â€“ vÃ  báº¡n Ä‘ang Ä‘i Ä‘Ãºng hÆ°á»›ng


#### ğŸ›  FIX Cá»¤ THá»‚

##### 1ï¸âƒ£ Táº O CLUSTER NHá» NHáº¤T

```bash
gcloud dataproc clusters create cdp-demo \
  --region asia-southeast1 \
  --master-machine-type e2-standard-2 \
  --worker-machine-type e2-standard-2 \
  --num-workers 2 \
  --master-boot-disk-size 50 \
  --worker-boot-disk-size 50 \
  --image-version 2.2-debian12 \
  --project cdp-dem-project
```

|ThÃ nh pháº§n|Dung lÆ°á»£ng|
|----------|----------|
|OS + Spark|~10â€“15GB|
|Log|~5GB|
|Job demo|<1GB|

**ğŸ’¡ Cluster nÃ y:**
*	2 worker
*	ráº»
*	Ä‘á»§ cháº¡y Spark demo

##### Bá» DataprocCreateBatchOperator dÃ¹ng DataprocSubmitJobOperator

Máº«u

```python
from airflow.providers.google.cloud.operators.dataproc import DataprocSubmitJobOperator

bronze_to_silver = DataprocSubmitJobOperator(
    task_id="bronze_to_silver",
    project_id=PROJECT_ID,
    region=REGION,
    job={
        "placement": {
            "cluster_name": "cdp-demo"
        },
        "pyspark_job": {
            "main_python_file_uri": "gs://cdp-dem-code/jobs/cdp/orders/cdp_orders_bronze_job.py",
            "args": [
                BRONZE_PATH,
                SILVER_PATH,
            ],
        },
    },
)
```

##### ğŸš€ CHáº Y SPARK JOB Báº°NG CLI

###### Bronze â†’ Silver
```bash
gcloud dataproc jobs submit pyspark \
  gs://cdp-dem-code/jobs/cdp/orders/cdp_orders_bronze_job.py \
  --cluster cdp-demo \
  --region asia-southeast1 \
  -- \
  gs://cdp-dem-bronze/orders \
  gs://cdp-dem-silver/orders
```

Check

```bash
gsutil ls gs://cdp-dem-silver/orders/
```

###### Silver â†’ Gold (BigQuery)

```bash
gcloud dataproc jobs submit pyspark \
  gs://cdp-dem-code/jobs/cdp/orders/cdp_orders_gold_job.py \
  --cluster cdp-demo \
  --region asia-southeast1 \
  -- \
  gs://cdp-dem-silver/orders \
  cdp-dem-project \
  cdp_gold \
  orders
```

Check BigQuery:

```bash
bq show cdp-dem-project:cdp_gold.orders
```

###### TEST DAG:

```bash
airflow dags test cdp_orders_bronze_to_gold 2025-12-19
```

##### ğŸ§¹ SAU KHI CHáº Y XONG DELETE

```bash
gcloud dataproc clusters delete cdp-demo \
  --region asia-southeast1 \
  --project cdp-dem-project
```

> BÆ°á»›c 5 thá»±c hiá»‡n cÃ³ phÃ¡t sinh lÃ´i do mÃ´i trÆ°á»ng, Ã i váº­y sáº½ chuyá»ƒn qua bÆ°á»›c 4 (thá»±c cháº¥t lÃ m lÃ m láº¡i bÆ°á»›c 5 vá»›i cÃ¡ch dÃ¹ng dataproc cluster)

## Step 6: Cloud Composer (Enterprise Airflow):

### ğŸ¯ Má»¥c tiÃªu:
*	Airflow cháº¡y tháº­t trÃªn GCP (khÃ´ng local)
*	DAG giá»‘ng há»‡t DAG báº¡n vá»«a test thÃ nh cÃ´ng
*	Spark cháº¡y trÃªn Dataproc Cluster (táº¡m thá»i â€“ phÃ¹ há»£p free quota)
*	Codespace = DEV
*	Composer = PROD orchestration

### ğŸ”’ NGUYÃŠN Táº®C:

> âŒ **KHÃ”NG** táº¡o Composer khi:

*	Dataproc cluster Ä‘ang cháº¡y khÃ´ng cáº§n thiáº¿t
*	ChÆ°a gÃ¡n Ä‘Ãºng Service Account

> âœ… **Composer** = tá»‘n tiá»n nháº¥t, nÃªn:

*	Táº¡o â†’ test â†’ xÃ³a
*	KhÃ´ng Ä‘á»ƒ cháº¡y qua Ä‘Ãªm

### ğŸ§± 6.0: PRE-CHECK

#### 1. XÃ¡c nháº­n tráº¡ng thÃ¡i hiá»‡n táº¡i:

```bash
# KhÃ´ng cÃ²n cluster cÅ©
gcloud dataproc clusters list \
  --region asia-southeast1 \
  --project cdp-dem-project

# KhÃ´ng cÃ²n composer
gcloud composer environments list \
  --locations asia-southeast1 \
  --project cdp-dem-project
```

ğŸ‘‰ Káº¿t quáº£ mong muá»‘n

```code
Listed 0 items.
```

#### 2. Billing an toÃ n:

```text
Billing â†’ Overview
```

> âœ” Remaining credit > 0
> âœ” Charges â‰ˆ 0

### 6.1 SERVICE ACCOUNT:

Composer TUYá»†T Äá»I khÃ´ng dÃ¹ng default SA.

#### 6.1.1 Táº¡o Service Account cho Composer:

```bash
gcloud iam service-accounts create cdp-composer-sa \
  --display-name "CDP Composer Service Account" \
  --project cdp-dem-project
```

**ğŸ“Œ SA:**

```text
cdp-composer-sa@cdp-dem-project.iam.gserviceaccount.com
```

#### 6.1.2 GÃ¡n IAM tá»‘i thiá»ƒu (BANK-GRADE):

```bash
PROJECT_ID=cdp-dem-project
COMPOSER_SA=cdp-composer-sa@$PROJECT_ID.iam.gserviceaccount.com
```

##### Composer worker

```bash
gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member="serviceAccount:$COMPOSER_SA" \
  --role="roles/composer.worker"
```

##### Dataproc

```bash
gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member="serviceAccount:$COMPOSER_SA" \
  --role="roles/dataproc.editor"
```

##### BigQuery

```bash
gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member="serviceAccount:$COMPOSER_SA" \
  --role="roles/bigquery.jobUser"

gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member="serviceAccount:$COMPOSER_SA" \
  --role="roles/bigquery.dataEditor"
```

##### GCS

```bash
gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member="serviceAccount:$COMPOSER_SA" \
  --role="roles/storage.objectAdmin"
```

#### 6.1.3 Composer Service Agent (HAY Bá»Š THIáº¾U â—):

```bash
PROJECT_NUMBER=585752501826
```

```bash
gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member="serviceAccount:service-${PROJECT_NUMBER}@cloudcomposer-accounts.iam.gserviceaccount.com" \
  --role="roles/composer.ServiceAgentV2Ext"
```

> ğŸ“Œ Náº¿u thiáº¿u role nÃ y â†’ Composer FAIL Ã¢m tháº§m

### 6.2 â˜ï¸ Táº O CLOUD COMPOSER (TIáº¾T KIá»†M):

#### 6.2.1 Chá»n version CHUáº¨N

|ThÃ nh pháº§n|Version|
|----------|-------|
|Composer|3|
|Airflow|2.10.5 âœ…|
|Python|3.10|
|Size|small|

#### 6.2.3 Lá»‡nh táº¡o Composer

â³ 20â€“30 phÃºt

```bash
gcloud composer environments create cdp-airflow \
  --location asia-southeast1 \
  --image-version composer-3-airflow-2.10.5 \
  --environment-size small \
  --service-account cdp-composer-sa@cdp-dem-project.iam.gserviceaccount.com \
  --project cdp-dem-project
```

#### 6.2.3 Theo dÃµi tráº¡ng thÃ¡i:

```bash
gcloud composer environments describe cdp-airflow \
  --location asia-southeast1 \
  --project cdp-dem-project \
  --format="value(state)"
```

âœ” RUNNING â†’ OK
âŒ ERROR â†’ Dá»ªNG

### 6.3 Äáº¨Y DAG LÃŠN COMPOSER

#### 6.3.1 Láº¥y DAG bucket:

```bash
gcloud composer environments describe cdp-airflow \
  --location asia-southeast1 \
  --project cdp-dem-project \
  --format="value(config.dagGcsPrefix)"
```

ğŸ“Œ VÃ­ dá»¥:

```code
gs://asia-southeast1-cdp-airflow-xxxx-bucket/dags
```

Káº¿t quáº£:

```code
gs://asia-southeast1-cdp-airflow-96b66680-bucket/dags
```

#### 6.3.2 Upload DAG

```bash
gsutil rsync -r .airflow/dags/cdp \
  gs://asia-southeast1-cdp-airflow-96b66680-bucket/dags/cdp
```

â± Sau ~30â€“60s DAG sáº½ xuáº¥t hiá»‡n

#### 6.3.3 Má»Ÿ Airflow UI

```bash
gcloud composer environments describe cdp-airflow \
  --location asia-southeast1 \
  --project cdp-dem-project \
  --format="value(config.airflowUri)"
```

ğŸ‘‰ Login â†’ tháº¥y DAG:

```code
cdp_orders_bronze_to_gold
```

### 6.4 AIRFLOW VARIABLES

**Airflow UI â†’ Admin â†’ Variables**

|Key|Value|
|---|-----|
|PROJECT_ID|cdp-dem-project|
|REGION|asia-southeast1|
|JOB_BUCKET|cdp-dem-code|
|BRONZE_PATH|gs://cdp-dem-bronze/orders|
|SILVER_PATH|gs://cdp-dem-silver/orders|
|BQ_DATASET|cdp_gold|
|BQ_TABLE|orders|

### 6.5 Táº O DATAPROC CLUSTER:

#### 1ï¸âƒ£ Táº¡o Dataproc cluster cdp-demo

```bash
gcloud dataproc clusters create cdp-demo \
  --region asia-southeast1 \
  --master-machine-type e2-standard-2 \
  --worker-machine-type e2-standard-2 \
  --num-workers 2 \
  --master-boot-disk-size 50 \
  --worker-boot-disk-size 50 \
  --image-version 2.2-debian12 \
  --project cdp-dem-project
```

#### 2ï¸âƒ£ Verify cluster

```bash
gcloud dataproc clusters list \
  --region asia-southeast1 \
  --project cdp-dem-project
```

âœ” Tháº¥y:

```code
cdp-demo   RUNNING
```

### 6.6 RUN DAG TRÃŠN GCP ğŸ‰

1ï¸âƒ£ Táº¡o Dataproc cluster (giá»‘ng lÃºc test local)

2ï¸âƒ£ Trigger DAG trÃªn UI

3ï¸âƒ£ Theo dÃµi:
* Dataproc â†’ Jobs
* BigQuery â†’ partition má»›i

### 6.7 KIá»‚M TRA Káº¾T QUÃ€

#### 1ï¸âƒ£ Kiá»ƒm tra tráº¡ng thÃ¡i DAG trong Composer:

**Trong Composer UI:**
*	DAG: cdp_orders_bronze_to_gold
*	Cáº£ 2 task:
	+	bronze_to_silver
	+	silver_to_gold

ğŸ‘‰ MÃ u xanh (SUCCESS)
ğŸ‘‰ KhÃ´ng cÃ²n retry / failed

âœ” Náº¿u Ä‘Ãºng â†’ sang bÆ°á»›c 2

#### 2ï¸âƒ£ Kiá»ƒm tra dá»¯ liá»‡u Silver (GCS):

```bash
gsutil ls gs://cdp-dem-silver/orders/
```

Káº¿t quáº£:

```code
gs://cdp-dem-silver/orders/
gs://cdp-dem-silver/orders/_SUCCESS
gs://cdp-dem-silver/orders/order_date=2024-01-01/
gs://cdp-dem-silver/orders/order_date=2024-01-02/
```

**ğŸ‘‰ Äiá»u nÃ y xÃ¡c nháº­n:**
*	Spark Bronze â†’ Silver cháº¡y OK
*	Partition theo order_date Ä‘Ãºng chuáº©n Lakehouse

#### 3ï¸âƒ£ Kiá»ƒm tra dá»¯ liá»‡u Gold (BigQuery)

##### 3.1 Kiá»ƒm tra table:

```bash
bq show cdp-dem-project:cdp_gold.orders
```

**Cáº§n tháº¥y:**
* Table tá»“n táº¡i
* Partition: DAY (field: order_date)
* CÃ³ Total Rows > 0

#### 3.2 Query nhanh Ä‘á»ƒ xÃ¡c nháº­n data

```sql
SELECT
  order_date,
  COUNT(*) AS cnt,
  SUM(amount) AS total_amount
FROM `cdp-dem-project.cdp_gold.orders`
GROUP BY order_date
ORDER BY order_date;
```

#### 4ï¸âƒ£ Kiá»ƒm tra cost:

```bash
gcloud dataproc clusters list \
  --region asia-southeast1 \
  --project cdp-dem-project
```



### 6.8 ğŸ§¹ CLEANUP 

```bash
# XÃ³a cluster
gcloud dataproc clusters delete cdp-demo \
  --region asia-southeast1 \
  --project cdp-dem-project

# (Optional) XÃ³a Composer
gcloud composer environments delete cdp-airflow \
  --location asia-southeast1 \
  --project cdp-dem-project
```

> ğŸ’° â†’ KHÃ”NG tá»‘n tiá»n qua Ä‘Ãªm

## Step 7: Testing with Batch Serverless

### âœ… 7.1 Chiáº¿n lÆ°á»£c Ä‘Ãºng cho Free Tier

#### ğŸ”¥ NGUYÃŠN Táº®C Sá»NG CÃ’N:

|Má»¥c|Quyáº¿t Ä‘á»‹nh|
|---|----------|
|Dataproc|Serverless Batch|
|Cluster|âŒ KhÃ´ng dÃ¹ng|
|runtime_config|âŒ KhÃ´ng set|
|executor|Google aut
|cores|Google auto|
|memory|Google auto|
|Batch|nhá», ngáº¯n|
|Cost|Pay-per-job (vÃ i cent)|

**ğŸ‘‰ Chá»‰ truyá»n Ä‘Ãºng 3 thá»©:**
* main_python_file_uri
* args
* project / region

### 7.2 ğŸ§© DAG CHUáº¨N DÃ™NG DATAPROC SERVERLESS (FREE TIER SAFE)

> ğŸ‘‰ Thay toÃ n bá»™ ***DataprocSubmitJobOperator*** báº±ng ***DataprocCreateBatchOperator***

#### âœ… DAG VERSION â€“ SERVERLESS SAFE

```python
from airflow import DAG
from airflow.providers.google.cloud.operators.dataproc import DataprocCreateBatchOperator
from airflow.models import Variable
from datetime import datetime

PROJECT_ID = Variable.get("PROJECT_ID")
REGION = Variable.get("REGION")

JOB_BUCKET = Variable.get("JOB_BUCKET")
BRONZE_PATH = Variable.get("BRONZE_PATH")
SILVER_PATH = Variable.get("SILVER_PATH")
BQ_DATASET = Variable.get("BQ_DATASET")
BQ_TABLE = Variable.get("BQ_TABLE")

with DAG(
    dag_id="cdp_orders_bronze_to_gold_serverless",
    start_date=datetime(2025, 12, 19),
    schedule=None,
    catchup=False,
    tags=["cdp", "orders", "dataproc", "serverless"],
) as dag:

    bronze_to_silver = DataprocCreateBatchOperator(
        task_id="bronze_to_silver",
        project_id=PROJECT_ID,
        region=REGION,
        batch={
            "pyspark_batch": {
                "main_python_file_uri": f"gs://{JOB_BUCKET}/jobs/cdp/orders/cdp_orders_bronze_job.py",
                "args": [
                    BRONZE_PATH,
                    SILVER_PATH,
                ],
            }
        },
    )

    silver_to_gold = DataprocCreateBatchOperator(
        task_id="silver_to_gold",
        project_id=PROJECT_ID,
        region=REGION,
        batch={
            "pyspark_batch": {
                "main_python_file_uri": f"gs://{JOB_BUCKET}/jobs/cdp/orders/cdp_orders_gold_job.py",
                "args": [
                    SILVER_PATH,
                    PROJECT_ID,
                    BQ_DATASET,
                    BQ_TABLE,
                ],
            }
        },
    )

    bronze_to_silver >> silver_to_gold
```

* ğŸ“Œ KhÃ´ng cÃ³ runtime_config
* ğŸ“Œ KhÃ´ng cÃ³ cluster_name
* ğŸ“Œ KhÃ´ng cÃ³ resource override

> â†’ Google tá»± cáº¥p resource tá»‘i thiá»ƒu há»£p lá»‡

### 7.3 Service Accounts

#### âœ… 7.3.1: XÃ¡c Ä‘á»‹nh Service Accounts

```bash
gcloud composer environments describe cdp-airflow \
  --location asia-southeast1 \
  --project cdp-dem-project \
  --format="value(config.nodeConfig.serviceAccount)"
```

Káº¿t quáº£:

```code
cdp-composer-sa@cdp-dem-project.iam.gserviceaccount.com
```

#### âœ… 7.3.2: Cáº¥p quyá»n Service Account User

```bash
gcloud iam service-accounts add-iam-policy-binding \
  585752501826-compute@developer.gserviceaccount.com \
  --member="serviceAccount:cdp-composer-sa@cdp-dem-project.iam.gserviceaccount.com" \
  --role="roles/iam.serviceAccountUser" \
  --project cdp-dem-project
```

#### âœ… 7.3.3: Cáº¥p quyá»n Dataproc Serverless (náº¿u chÆ°a)

```bash
gcloud projects add-iam-policy-binding cdp-dem-project \
  --member="serviceAccount:cdp-composer-sa@cdp-dem-project.iam.gserviceaccount.com" \
  --role="roles/dataproc.editor"
```

#### 7.3.4 Kiá»ƒm tra:

```bash
gcloud iam service-accounts get-iam-policy \
  585752501826-compute@developer.gserviceaccount.com \
  --project cdp-dem-project
```

### 7.4 ğŸ§ª TEST CÃCH ÄÃšNG (KHÃ”NG Tá»N TIá»€N TREO)

#### 7.4.1 Sync DAG lÃªn Composer

```bash
gsutil rsync -r .airflow/dags/cdp \
  gs://asia-southeast1-cdp-airflow-96b66680-bucket/dags/cdp
```

#### 7.4.2 Trigger DAG trong UI (khuyáº¿n nghá»‹):

ğŸ‘‰ Composer UI â†’ Trigger
â›” KhÃ´ng dÃ¹ng airflow dags test cho serverless

#### 7.4.3 Theo dÃµi batch

```bash
gcloud dataproc batches list \
  --region asia-southeast1 \
  --project cdp-dem-project
```

#### 7.4.4 Xem log batch:

```bash
gcloud dataproc batches describe <BATCH_ID> \
  --region asia-southeast1 \
  --project cdp-dem-project
```

```bash
gcloud dataproc batches describe 49771ca0-cd25-4a0b-a16d-1a219152890e \
  --region asia-southeast1 \
  --project cdp-dem-project
```

### 7.5 ğŸ’° CAM Káº¾T CHI PHÃ (Ráº¤T QUAN TRá»ŒNG)

|ThÃ nh pháº§n|CÃ³ tá»‘n tiá»n khÃ´ng|
|----------|-----------------|
|Composer|CÃ³ (nhÆ°ng ráº¥t tháº¥p, free credit cover)|
|Dataproc Serverless Batch|CÃ³ (vÃ i cent / job)|
|GCS|Ráº»|
|BigQuery|Free tier Ä‘á»§|

ğŸ‘‰ KhÃ´ng cÃ³ VM treo
ğŸ‘‰ KhÃ´ng cÃ³ cluster sá»‘ng
ğŸ‘‰ KhÃ´ng cÃ³ surprise bill



