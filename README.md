# spark401-codespace

## setup environment: 

Auto activate .venv
```bash
cat <<'EOF' >> ~/.bashrc

# === Auto activate project venv ===
if [ -f "/workspaces/spark401-codespace/.venv/bin/activate" ]; then
  source /workspaces/spark401-codespace/.venv/bin/activate
fi
EOF
```

check spark:
```bash
find /opt -maxdepth 2 -type d | grep spark
```

setup PATH

```bash
cat <<'EOF' >> ~/.bashrc

# === Spark env ===
export SPARK_HOME=/opt/spark
export PATH=$SPARK_HOME/bin:$PATH
EOF
```

reload shell
```bash
source ~/.bashrc
```

## setup Jupyter:

Jupyter and Spark on codeSapce for testing

start

```bash
python -m jupyterlab --ip=0.0.0.0 --port=8888 --no-browser
```

setup bypass token jupyter

```bash
jupyter lab --generate-config
```
>>> ~/.jupyter/jupyter_lab_config.py

```bash
code ~/.jupyter/jupyter_lab_config.py
```

```python
c.ServerApp.ip = "0.0.0.0"
c.ServerApp.port = 8888

# üî• Disable token & password
c.ServerApp.token = ""
c.ServerApp.password = ""

# Kh√¥ng m·ªü browser trong container
c.ServerApp.open_browser = False

# Cho ph√©p ch·∫°y d∆∞·ªõi root / container
c.ServerApp.allow_root = True
```

sau khi set no token chi c·∫ßn start

```bash
python -m jupyterlab
```

TƒÉt token Jupyter t·∫°m th·ªùi

```bash
python -m jupyterlab \
  --ip=0.0.0.0 \
  --port=8888 \
  --no-browser \
  --ServerApp.token='' \
  --ServerApp.password=''
```

set jupyter as auto

```bash
# Auto start Jupyter (optional)
if [ -z "$JUPYTER_STARTED" ]; then
  export JUPYTER_STARTED=1
  nohup python -m jupyterlab >/tmp/jupyter.log 2>&1 &
fi
```

## Setup Spark

setup kernel
```bash
pip install ipykernel
```

ƒêƒÉng k√Ω kernal Spark cho Jupyter:

```bash
python -m ipykernel install \
  --user \
  --name spark401 \
  --display-name "PySpark 4.0.1 (.venv)"
```

start and expose spark

```bash 
pyspark --conf spark.ui.port=4040 

```

# Setup GCP:

```
Codespaces
 ‚îú‚îÄ code PySpark
 ‚îú‚îÄ demo Spark UI (local)
 ‚îú‚îÄ git push
      ‚Üì
GCP
 ‚îú‚îÄ GCS (Bronze/Silver/Gold)
 ‚îú‚îÄ Dataproc (Spark cluster)
 ‚îú‚îÄ Airflow (VM / Composer)
 ‚îî‚îÄ BigQuery (Gold serving)
```

B∆∞·ªõc 1: install on codespace

```bash
curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg \
  | sudo gpg --dearmor -o /usr/share/keyrings/cloud.google.gpg

echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main" \
  | sudo tee /etc/apt/sources.list.d/google-cloud-sdk.list

sudo apt-get update
sudo apt-get install -y google-cloud-cli
```

Check version

```bash
gcloud version
```

B∆∞·ªõc 2: login GCP from codespace

```bash
gcloud auth login
```

B∆∞·ªõc 3: setup project

```bash
gcloud config set project cdp-dem-project
```

check

```bash
gcloud config list
```

B∆∞·ªõc 4: enable API

```bash
gcloud services enable \
  storage.googleapis.com \
  dataproc.googleapis.com \
  compute.googleapis.com \
  iam.googleapis.com \
  bigquery.googleapis.com
```

B∆∞·ªõc 5: Create BRONZE/SILVER/GOLD Bucket:

```bash
gsutil mb -p cdp-dem-project -l asia-southeast1 gs://cdp-dem-bronze
gsutil mb -p cdp-dem-project -l asia-southeast1 gs://cdp-dem-silver
gsutil mb -p cdp-dem-project -l asia-southeast1 gs://cdp-dem-gold
```

check:

```bash
gsutil ls
```

# Session 1: test on job

Step 1: submit job

```bash
spark-submit jobs/bronze_to_silver.py \
  data/bronze_sample.csv \
  data/silver_out
```

check:

```bash
ls data/silver_out
```

fast test:

```bash
python - <<'EOF'
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
spark.read.parquet("data/silver_out").show()
spark.stop()
EOF
```

Step 2: to GCP

```bash
spark-submit jobs/bronze_to_silver.py \
  gs://cdp-dem-bronze/orders/bronze_sample.csv \
  gs://cdp-dem-silver/orders
```

# Session 2: Dataproc cluster + submit job th·∫≠t

Step 1: setup evironment:

```bash
export PROJECT_ID=cdp-dem-project
export REGION=asia-southeast1
export ZONE=asia-southeast1-a
export CLUSTER_NAME=cdp-demo-dp
```
Step 2: Create Dataproc cluster (NH·∫∏ ‚Äì TI·∫æT KI·ªÜM)

service account:

```
585752501826-compute@developer.gserviceaccount.com
```
## Ph√¢n quy·ªÅn: 
* Dataproc Worker:

```bash
gcloud projects add-iam-policy-binding cdp-dem-project \
  --member="serviceAccount:585752501826-compute@developer.gserviceaccount.com" \
  --role="roles/dataproc.worker"
```

* Storage Admin (ƒë·ªçc/ghi GCS Bronze/Silver):

```bash
gcloud projects add-iam-policy-binding cdp-dem-project \
  --member="serviceAccount:585752501826-compute@developer.gserviceaccount.com" \
  --role="roles/storage.admin"
```

* Logging + Monitoring:
```bash
gcloud projects add-iam-policy-binding cdp-dem-project \
  --member="serviceAccount:585752501826-compute@developer.gserviceaccount.com" \
  --role="roles/logging.logWriter"

gcloud projects add-iam-policy-binding cdp-dem-project \
  --member="serviceAccount:585752501826-compute@developer.gserviceaccount.com" \
  --role="roles/monitoring.metricWriter"
``` 

check:

```bash
gcloud projects get-iam-policy cdp-dem-project \
  --flatten="bindings[].members" \
  --filter="bindings.members:585752501826-compute@developer.gserviceaccount.com" \
  --format="table(bindings.role)"
```

* Create with custom:

```bash
gcloud dataproc clusters create $CLUSTER_NAME \
  --region $REGION \
  --zone $ZONE \
  --master-machine-type e2-standard-2 \
  --worker-machine-type e2-standard-2 \
  --num-workers 2 \
  --master-boot-disk-size 100 \
  --worker-boot-disk-size 100 \
  --image-version 2.2-debian12 \
  --project $PROJECT_ID
```
Step 2: ƒê∆∞a job l√™n GCP:


T·∫°o bucket ch·ª©a code (n·∫øu ch∆∞a c√≥):
```bash
gsutil mb -p $PROJECT_ID -l $REGION gs://cdp-dem-code || true
```

Upload file job:
```bash
gsutil cp jobs/bronze_to_silver.py gs://cdp-dem-code/jobs/bronze_to_silver.py
```

## Data to Bronze

Step 3: Chu·∫©n b·ªã d·ªØ li·ªáu Bronze tr√™n GCS

upload data:
```bash
gsutil cp data/bronze_sample.csv gs://cdp-dem-bronze/orders/bronze_sample.csv
```
check:
```bash
gsutil ls gs://cdp-dem-bronze/orders
```

step 4: test submit job

```bash
gcloud dataproc jobs submit pyspark \
  gs://cdp-dem-code/jobs/bronze_to_silver.py \
  --region $REGION \
  --cluster $CLUSTER_NAME \
  -- \
  gs://cdp-dem-bronze/orders/bronze_sample.csv \
  gs://cdp-dem-silver/orders
```

follow logs:
```bash
gcloud dataproc jobs list --region $REGION
```

check detail job ID:

```bash
gcloud dataproc jobs describe <JOB_ID> --region $REGION
```

check silver

```bash
gsutil ls gs://cdp-dem-silver/orders
```

check data via script:

```bash
python - <<'EOF'
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
spark.read.parquet("gs://cdp-dem-silver/orders").show()
spark.stop()
EOF
```

## Bronze -> Silver -> Gold:

```
Bronze (GCS)
   ‚Üì
Silver (GCS Parquet)
   ‚Üì
Gold (BigQuery Tables)
```

Step 5: T·∫†O DATASET BIGQUERY:

```bash
bq mk \
  --location=$REGION \
  --dataset $PROJECT_ID:cdp_gold
```

```bash
bq mk \
  --location=asia-southeast1 \
  --dataset cdp-dem-project:cdp_gold
```

check

```bash
bq ls cdp-dem-project:cdp_gold
```

t·∫°o staging:

```bash
gsutil mb -p cdp-dem-project -l asia-southeast1 gs://cdp-dem-bq-temp
```

step 7: grant permission

x√°c ƒëinh acct
```bash
gcloud dataproc clusters describe cdp-demo-dp \
  --region asia-southeast1 \
  --format="value(config.gceClusterConfig.serviceAccount)"
```
grant:

```bash
gcloud projects add-iam-policy-binding cdp-dem-project \
  --member="serviceAccount:585752501826-compute@developer.gserviceaccount.com" \
  --role="roles/bigquery.dataEditor"

gcloud projects add-iam-policy-binding cdp-dem-project \
  --member="serviceAccount:585752501826-compute@developer.gserviceaccount.com" \
  --role="roles/bigquery.jobUser"
```

chu·∫©n h∆°n:

```bash
bq update \
  --dataset \
  --add_iam_member="serviceAccount:585752501826-compute@developer.gserviceaccount.com:roles/bigquery.dataEditor" \
  cdp-dem-project:cdp_gold
```

step 7: upload job to GCP

```bash
gsutil cp jobs/silver_to_gold.py gs://cdp-dem-code/jobs/silver_to_gold.py
```


step 8: submit gold job

```bash
gcloud dataproc jobs submit pyspark \
  gs://cdp-dem-code/jobs/silver_to_gold.py \
  --region asia-southeast1 \
  --cluster cdp-demo-dp \
  -- \
  gs://cdp-dem-silver/orders \
  cdp-dem-project \
  cdp_gold \
  orders
```

check result:
```bash
bq ls cdp-dem-project:cdp_gold
```
query
```bash
bq query --use_legacy_sql=false '
SELECT * FROM `cdp-dem-project.cdp_gold.orders`
'
```

## Airflow DAG orchestrate Bronze ‚Üí Silver ‚Üí Gold:

```
Bronze (GCS) 
   ‚Üí Spark on Dataproc (Silver)
      ‚Üí Spark BigQuery (Gold)
```

‚Ä¢	Airflow KH√îNG ch·∫°y Spark tr·ª±c ti·∫øp
	‚Ä¢	Airflow ch·ªâ submit job + monitor
	‚Ä¢	T√°ch r√µ:
	‚Ä¢	Code Spark = jobs/
	‚Ä¢	Orchestration = dags/

Architecture:

```
Airflow (Composer / Local)
 ‚îú‚îÄ‚îÄ DataprocCreateClusterOperator (optional)
 ‚îú‚îÄ‚îÄ DataprocSubmitJobOperator (bronze_to_silver)
 ‚îú‚îÄ‚îÄ DataprocSubmitJobOperator (silver_to_gold)
 ‚îî‚îÄ‚îÄ DataprocDeleteClusterOperator (cost saving)
```

project:

```
spark401-codespace/
‚îú‚îÄ‚îÄ .airflow/dags/
‚îÇ   ‚îî‚îÄ‚îÄ cdp_bronze_silver_gold.py
‚îú‚îÄ‚îÄ jobs/
‚îÇ   ‚îú‚îÄ‚îÄ bronze_to_silver.py
‚îÇ   ‚îî‚îÄ‚îÄ silver_to_gold.py
‚îú‚îÄ‚îÄ data/
‚îú‚îÄ‚îÄ README.md
```

flow

```
Codespace (dev)
 ‚îî‚îÄ‚îÄ Airflow local (test DAG logic)
       ‚îî‚îÄ‚îÄ Submit Spark job ‚Üí Dataproc (GCP)
             ‚îî‚îÄ‚îÄ GCS / BigQuery
```

### Airflow (local) in codespace:

**Chi·∫øn l∆∞·ª£c**
|Th√†nh ph·∫ßn|Vai tr√≤|
|----------|-------|
|Codespace|Code, test, CI/CD|
|Dataproc|Spark runtime (on-demand)|
|Composer|Airflow production|
|GCS|Bronze / Silver|
|BigQuery|Gold|



* read doc at [link](./airflow/README.md) how to setup airflow in codespace

setup cloud: ADC
```bash
gcloud auth application-default login
```
|L·ªánh|D√πng cho|
|----|---------|
|gcloud auth login|CLI, gsutil, gcloud|
|gcloud auth application-default login|Python / Airflow / SDK / Spark|

Airflow KH√îNG d√πng credential c·ªßa gcloud auth login
‚Üí n√≥ ch·ªâ ƒë·ªçc ADC


run test
```bash
airflow dags test bronze_to_silver 2024-01-01
```

```bash
airflow dags test silver_to_gold 2024-01-01
```

test submit job tr·ª±c ti·∫øp:
```bash
gcloud dataproc jobs list \
  --region asia-southeast1 \
  --cluster cdp-demo-dp
```



## Delete cluster sau khi test:

|H√†nh ƒë·ªông|Chi ph√≠|
|---------|-------|
|Cluster t·ªìn t·∫°i|üí∏ t·ªën ti·ªÅn|
|Delete cluster|‚ùå kh√¥ng t·ªën|
|Submit job khi cluster kh√¥ng t·ªìn t·∫°i|‚ùå fail|

üëâ V√¨ v·∫≠y sau khi d√πng xong c·∫ßn delete cluster, v√† khi test c·∫ßn:

üîπ Khi c·∫ßn ch·∫°y DAG ‚Üí t·∫°o l·∫°i

üîπ Kh√¥ng c·∫ßn t√™n m·ªõi (t√™n c≈© d√πng l·∫°i OK)

check
```bash
gcloud dataproc clusters list --region=asia-southeast1
```
xem chi ti·∫øt cluster:
```bash
gcloud dataproc clusters describe cdp-demo-dp \
  --region=asia-southeast1
```

check instances
```bash
gcloud compute instances list
```

check disks
```bash
gcloud compute disks list
```

check staticIP
```bash
gcloud compute addresses list
```

check dataproc jobs
```bash
gcloud dataproc jobs list --region=asia-southeast1
```

### ti·∫øt ki·ªám
delete
```bash
gcloud dataproc clusters delete $CLUSTER_NAME --region $REGION
```

## Khi c·∫ßn test l·∫°i

run 

```bash
export PROJECT_ID=cdp-dem-project
export REGION=asia-southeast1
export ZONE=asia-southeast1-a
export CLUSTER_NAME=cdp-demo-dp
```

create:

```bash
gcloud dataproc clusters create $CLUSTER_NAME \
  --region $REGION \
  --zone $ZONE \
  --master-machine-type e2-standard-2 \
  --worker-machine-type e2-standard-2 \
  --num-workers 2 \
  --master-boot-disk-size 100 \
  --worker-boot-disk-size 100 \
  --image-version 2.2-debian12 \
  --project $PROJECT_ID
```

Ti·∫øt ki·ªám h∆°n n√™n t·∫°o disk 50GB:
```bash
export PROJECT_ID=cdp-dem-project
export REGION=asia-southeast1
export ZONE=asia-southeast1-a
export CLUSTER_NAME=cdp-demo-dp

gcloud dataproc clusters create $CLUSTER_NAME \
  --region $REGION \
  --zone $ZONE \
  --master-machine-type e2-standard-2 \
  --worker-machine-type e2-standard-2 \
  --num-workers 2 \
  --master-boot-disk-size 50 \
  --worker-boot-disk-size 50 \
  --image-version 2.2-debian12 \
  --project $PROJECT_ID
```