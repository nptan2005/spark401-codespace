{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3376a30f-8e1d-4b8d-b273-629ddb187f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/04 08:36:28 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---------+\n",
      "| ID|   Name|  Company|\n",
      "+---+-------+---------+\n",
      "|  1| sravan|company 1|\n",
      "|  2| ojaswi|company 1|\n",
      "|  3| rohith|company 2|\n",
      "|  4|sridevi|company 1|\n",
      "|  5|  bobby|company 1|\n",
      "+---+-------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate() # Init SparkSession\n",
    "\n",
    "d = [[\"1\", \"sravan\", \"company 1\"],\n",
    "        [\"2\", \"ojaswi\", \"company 1\"], \n",
    "        [\"3\", \"rohith\", \"company 2\"],\n",
    "        [\"4\", \"sridevi\", \"company 1\"], \n",
    "        [\"5\", \"bobby\", \"company 1\"]]\n",
    "\n",
    "cols = ['ID', 'Name', 'Company']\n",
    "df = spark.createDataFrame(d, cols)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cf2f217-fe47-42c0-9221-cb8d14cda802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n",
      "| ID|salary|department|\n",
      "+---+------+----------+\n",
      "|  1| 45000|        IT|\n",
      "|  2|145000|   Manager|\n",
      "|  6| 45000|        HR|\n",
      "|  5| 34000|     Sales|\n",
      "+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate() # Init SparkSession\n",
    "\n",
    "d1 = [[\"1\", \"45000\", \"IT\"],\n",
    "         [\"2\", \"145000\", \"Manager\"],\n",
    "         [\"6\", \"45000\", \"HR\"],\n",
    "         [\"5\", \"34000\", \"Sales\"]]\n",
    "\n",
    "cols = ['ID', 'salary', 'department']\n",
    "df1 = spark.createDataFrame(d1, cols)\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95f359ee-bab2-435a-a0e9-a01e14e05cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/04 08:39:10 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+---+------+----------+\n",
      "| id|  name|  company| id|salary|department|\n",
      "+---+------+---------+---+------+----------+\n",
      "|  1|sravan|company 1|  1| 45000|        IT|\n",
      "|  2|ojaswi|company 1|  2|145000|   Manager|\n",
      "|  5| bobby|company 1|  5| 34000|     Sales|\n",
      "+---+------+---------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('join_example').getOrCreate()\n",
    "\n",
    "# first DataFrame\n",
    "d1 = [(\"1\", \"sravan\", \"company 1\"),\n",
    "      (\"2\", \"ojaswi\", \"company 1\"),\n",
    "      (\"3\", \"rohith\", \"company 2\"),\n",
    "      (\"4\", \"sridevi\", \"company 1\"),\n",
    "      (\"5\", \"bobby\", \"company 1\")]\n",
    "cols1 = ['id', 'name', 'company']\n",
    "df1 = spark.createDataFrame(d1, cols1)\n",
    "\n",
    "# second DataFrame\n",
    "d2 = [(\"1\", \"45000\", \"IT\"),\n",
    "      (\"2\", \"145000\", \"Manager\"),\n",
    "      (\"6\", \"45000\", \"HR\"),\n",
    "      (\"5\", \"34000\", \"Sales\")]\n",
    "cols2 = ['id', 'salary', 'department']\n",
    "df2 = spark.createDataFrame(d2, cols2)\n",
    "\n",
    "df1.join(df2, df1.id == df2.id,'inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75cf931d-7f88-4740-8b90-12ef38a2fde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---------+----+------+----------+\n",
      "|  ID|   NAME|  Company|  ID|salary|department|\n",
      "+----+-------+---------+----+------+----------+\n",
      "|   1| sravan|company 1|   1| 45000|        IT|\n",
      "|   2| ojaswi|company 1|   2|145000|   Manager|\n",
      "|   3| rohith|company 2|NULL|  NULL|      NULL|\n",
      "|   4|sridevi|company 1|NULL|  NULL|      NULL|\n",
      "|   5|  bobby|company 1|   5| 34000|     Sales|\n",
      "|NULL|   NULL|     NULL|   6| 45000|        HR|\n",
      "+----+-------+---------+----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
    "\n",
    "# first dataframe data\n",
    "d1 = [[\"1\", \"sravan\", \"company 1\"],\n",
    "      [\"2\", \"ojaswi\", \"company 1\"],\n",
    "      [\"3\", \"rohith\", \"company 2\"],\n",
    "      [\"4\", \"sridevi\", \"company 1\"],\n",
    "      [\"5\", \"bobby\", \"company 1\"]]\n",
    "cols1 = ['ID', 'NAME', 'Company']\n",
    "df1 = spark.createDataFrame(d1, cols1)\n",
    "\n",
    "# second dataframe data\n",
    "d2 = [[\"1\", \"45000\", \"IT\"],\n",
    "      [\"2\", \"145000\", \"Manager\"],\n",
    "      [\"6\", \"45000\", \"HR\"],\n",
    "      [\"5\", \"34000\", \"Sales\"]]\n",
    "cols2 = ['ID', 'salary', 'department']\n",
    "df2 = spark.createDataFrame(d2, cols2)\n",
    "\n",
    "df1.join(df2, df1.ID == df2.ID, \"outer\").show()\n",
    "\n",
    "# df2.join(df1, df1.ID == df2.ID, \"outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9afd10c8-d879-48ea-a1e2-17686103f0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---------+----+------+----------+\n",
      "|  ID|   NAME|  Company|  ID|salary|department|\n",
      "+----+-------+---------+----+------+----------+\n",
      "|   1| sravan|company 1|   1| 45000|        IT|\n",
      "|   2| ojaswi|company 1|   2|145000|   Manager|\n",
      "|   3| rohith|company 2|NULL|  NULL|      NULL|\n",
      "|   4|sridevi|company 1|NULL|  NULL|      NULL|\n",
      "|   5|  bobby|company 1|   5| 34000|     Sales|\n",
      "|NULL|   NULL|     NULL|   6| 45000|        HR|\n",
      "+----+-------+---------+----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
    "\n",
    "# first dataframe data\n",
    "d1 = [[\"1\", \"sravan\", \"company 1\"],\n",
    "      [\"2\", \"ojaswi\", \"company 1\"],\n",
    "      [\"3\", \"rohith\", \"company 2\"],\n",
    "      [\"4\", \"sridevi\", \"company 1\"],\n",
    "      [\"5\", \"bobby\", \"company 1\"]]\n",
    "cols1 = ['ID', 'NAME', 'Company']\n",
    "df1 = spark.createDataFrame(d1, cols1)\n",
    "\n",
    "# second dataframe data\n",
    "d2 = [[\"1\", \"45000\", \"IT\"],\n",
    "      [\"2\", \"145000\", \"Manager\"],\n",
    "      [\"6\", \"45000\", \"HR\"],\n",
    "      [\"5\", \"34000\", \"Sales\"]]\n",
    "cols2 = ['ID', 'salary', 'department']\n",
    "df2 = spark.createDataFrame(d2, cols2)\n",
    "\n",
    "df1.join(df2, df1.ID == df2.ID, \"full\").show()\n",
    "\n",
    "# df2.join(df1, df1.ID == df2.ID, \"full\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "490cac69-45b7-43f2-9064-4addbaf21fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---------+----+------+----------+\n",
      "|  ID|   NAME|  Company|  ID|salary|department|\n",
      "+----+-------+---------+----+------+----------+\n",
      "|   1| sravan|company 1|   1| 45000|        IT|\n",
      "|   2| ojaswi|company 1|   2|145000|   Manager|\n",
      "|   3| rohith|company 2|NULL|  NULL|      NULL|\n",
      "|   4|sridevi|company 1|NULL|  NULL|      NULL|\n",
      "|   5|  bobby|company 1|   5| 34000|     Sales|\n",
      "|NULL|   NULL|     NULL|   6| 45000|        HR|\n",
      "+----+-------+---------+----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
    "\n",
    "# First dataframe data\n",
    "d1 = [[\"1\", \"sravan\", \"company 1\"],\n",
    "      [\"2\", \"ojaswi\", \"company 1\"],\n",
    "      [\"3\", \"rohith\", \"company 2\"],\n",
    "      [\"4\", \"sridevi\", \"company 1\"],\n",
    "      [\"5\", \"bobby\", \"company 1\"]]\n",
    "cols1 = ['ID', 'NAME', 'Company']\n",
    "df1 = spark.createDataFrame(d1, cols1)\n",
    "\n",
    "# Second dataframe data\n",
    "d2 = [[\"1\", \"45000\", \"IT\"],\n",
    "      [\"2\", \"145000\", \"Manager\"],\n",
    "      [\"6\", \"45000\", \"HR\"],\n",
    "      [\"5\", \"34000\", \"Sales\"]]\n",
    "cols2 = ['ID', 'salary', 'department']\n",
    "df2 = spark.createDataFrame(d2, cols2)\n",
    "\n",
    "df1.join(df2, df1.ID == df2.ID, \"fullouter\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60423ac9-db04-44c0-8423-6d9d5c2ba8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---------+----+------+----------+\n",
      "| ID|   NAME|  Company|  ID|salary|department|\n",
      "+---+-------+---------+----+------+----------+\n",
      "|  1| sravan|company 1|   1| 45000|        IT|\n",
      "|  2| ojaswi|company 1|   2|145000|   Manager|\n",
      "|  3| rohith|company 2|NULL|  NULL|      NULL|\n",
      "|  4|sridevi|company 1|NULL|  NULL|      NULL|\n",
      "|  5|  bobby|company 1|   5| 34000|     Sales|\n",
      "+---+-------+---------+----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
    "\n",
    "# First dataframe data\n",
    "d1 = [[\"1\", \"sravan\", \"company 1\"], \n",
    "      [\"2\", \"ojaswi\", \"company 1\"], \n",
    "      [\"3\", \"rohith\", \"company 2\"],\n",
    "      [\"4\", \"sridevi\", \"company 1\"], \n",
    "      [\"5\", \"bobby\", \"company 1\"]]\n",
    "cols1 = ['ID', 'NAME', 'Company']\n",
    "df1 = spark.createDataFrame(d1, cols1)\n",
    "\n",
    "# Second dataframe data\n",
    "d2 = [[\"1\", \"45000\", \"IT\"], \n",
    "      [\"2\", \"145000\", \"Manager\"], \n",
    "      [\"6\", \"45000\", \"HR\"],\n",
    "      [\"5\", \"34000\", \"Sales\"]]\n",
    "cols2 = ['ID', 'salary', 'department']\n",
    "df2 = spark.createDataFrame(d2, cols2)\n",
    "\n",
    "df1.join(df2, df1.ID == df2.ID, \"left\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4c42519-f8e6-412b-a42a-22aa925f760c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---------+----+------+----------+\n",
      "| ID|   NAME|  Company|  ID|salary|department|\n",
      "+---+-------+---------+----+------+----------+\n",
      "|  1| sravan|company 1|   1| 45000|        IT|\n",
      "|  2| ojaswi|company 1|   2|145000|   Manager|\n",
      "|  3| rohith|company 2|NULL|  NULL|      NULL|\n",
      "|  4|sridevi|company 1|NULL|  NULL|      NULL|\n",
      "|  5|  bobby|company 1|   5| 34000|     Sales|\n",
      "+---+-------+---------+----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
    "\n",
    "# first dataframe\n",
    "d1 = [[\"1\", \"sravan\", \"company 1\"],\n",
    "      [\"2\", \"ojaswi\", \"company 1\"],\n",
    "      [\"3\", \"rohith\", \"company 2\"],\n",
    "      [\"4\", \"sridevi\", \"company 1\"],\n",
    "      [\"5\", \"bobby\", \"company 1\"]]\n",
    "cols1 = ['ID', 'NAME', 'Company']\n",
    "df1 = spark.createDataFrame(d1, cols1)\n",
    "\n",
    "# second dataframe\n",
    "d2 = [[\"1\", \"45000\", \"IT\"],\n",
    "      [\"2\", \"145000\", \"Manager\"],\n",
    "      [\"6\", \"45000\", \"HR\"],\n",
    "      [\"5\", \"34000\", \"Sales\"]]\n",
    "cols2 = ['ID', 'salary', 'department']\n",
    "df2 = spark.createDataFrame(d2, cols2)\n",
    "\n",
    "df1.join(df2, df1.ID == df2.ID, \"leftouter\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1b4caf5-1c32-47f4-8f54-6b52923423d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+---------+---+------+----------+\n",
      "|  ID|  NAME|  Company| ID|salary|department|\n",
      "+----+------+---------+---+------+----------+\n",
      "|   1|sravan|company 1|  1| 45000|        IT|\n",
      "|   2|ojaswi|company 1|  2|145000|   Manager|\n",
      "|NULL|  NULL|     NULL|  6| 45000|        HR|\n",
      "|   5| bobby|company 1|  5| 34000|     Sales|\n",
      "+----+------+---------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
    "\n",
    "# Data for left dataframe\n",
    "d1 = [[\"1\", \"sravan\", \"company 1\"], \n",
    "      [\"2\", \"ojaswi\", \"company 1\"],\n",
    "      [\"3\", \"rohith\", \"company 2\"],\n",
    "      [\"4\", \"sridevi\", \"company 1\"],\n",
    "      [\"5\", \"bobby\", \"company 1\"]]\n",
    "cols1 = ['ID', 'NAME', 'Company']\n",
    "df1 = spark.createDataFrame(d1, cols1)\n",
    "\n",
    "# Data for right dataframe\n",
    "d2 = [[\"1\", \"45000\", \"IT\"], \n",
    "      [\"2\", \"145000\", \"Manager\"], \n",
    "      [\"6\", \"45000\", \"HR\"],\n",
    "      [\"5\", \"34000\", \"Sales\"]]\n",
    "cols2 = ['ID', 'salary', 'department']\n",
    "df2 = spark.createDataFrame(d2, cols2)\n",
    "\n",
    "df1.join(df2, df1.ID == df2.ID, \"right\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2696ecc-ee04-47f3-9ab3-d878372ad33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+---------+---+------+----------+\n",
      "|  ID|  NAME|  Company| ID|salary|department|\n",
      "+----+------+---------+---+------+----------+\n",
      "|   1|sravan|company 1|  1| 45000|        IT|\n",
      "|   2|ojaswi|company 1|  2|145000|   Manager|\n",
      "|NULL|  NULL|     NULL|  6| 45000|        HR|\n",
      "|   5| bobby|company 1|  5| 34000|     Sales|\n",
      "+----+------+---------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
    "\n",
    "# first dataframe data and columns\n",
    "d1 = [[\"1\", \"sravan\", \"company 1\"],\n",
    "      [\"2\", \"ojaswi\", \"company 1\"],\n",
    "      [\"3\", \"rohith\", \"company 2\"],\n",
    "      [\"4\", \"sridevi\", \"company 1\"],\n",
    "      [\"5\", \"bobby\", \"company 1\"]]\n",
    "col1 = ['ID', 'NAME', 'Company']\n",
    "df1 = spark.createDataFrame(d1, col1)\n",
    "\n",
    "# second dataframe data and columns\n",
    "d2 = [[\"1\", \"45000\", \"IT\"],\n",
    "      [\"2\", \"145000\", \"Manager\"],\n",
    "      [\"6\", \"45000\", \"HR\"],\n",
    "      [\"5\", \"34000\", \"Sales\"]]\n",
    "col2 = ['ID', 'salary', 'department']\n",
    "df2 = spark.createDataFrame(d2, col2)\n",
    "\n",
    "df1.join(df2, df1.ID == df2.ID, \"rightouter\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b38283e-4055-41b1-966d-a6cb7ff075e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+\n",
      "| ID|  NAME|  Company|\n",
      "+---+------+---------+\n",
      "|  1|sravan|company 1|\n",
      "|  2|ojaswi|company 1|\n",
      "|  5| bobby|company 1|\n",
      "+---+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
    "\n",
    "# employee data\n",
    "d1 = [[\"1\", \"sravan\", \"company 1\"],\n",
    "      [\"2\", \"ojaswi\", \"company 1\"],\n",
    "      [\"3\", \"rohith\", \"company 2\"],\n",
    "      [\"4\", \"sridevi\", \"company 1\"], \n",
    "      [\"5\", \"bobby\", \"company 1\"]]\n",
    "cols1 = ['ID', 'NAME', 'Company']\n",
    "df1 = spark.createDataFrame(d1, cols1)\n",
    "\n",
    "# salary & department data\n",
    "d2 = [[\"1\", \"45000\", \"IT\"],\n",
    "      [\"2\", \"145000\", \"Manager\"], \n",
    "      [\"6\", \"45000\", \"HR\"],\n",
    "      [\"5\", \"34000\", \"Sales\"]]\n",
    "cols2 = ['ID', 'salary', 'department']\n",
    "df2 = spark.createDataFrame(d2, cols2)\n",
    "\n",
    "df1.join(df2, df1.ID == df2.ID, \"leftsemi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55933bf5-b304-4f4b-9322-85022e76c519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---------+\n",
      "| ID|   NAME|  COMPANY|\n",
      "+---+-------+---------+\n",
      "|  3| rohith|company 2|\n",
      "|  4|sridevi|company 1|\n",
      "+---+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
    "\n",
    "d1 = [[\"1\", \"sravan\", \"company 1\"],\n",
    "      [\"2\", \"ojaswi\", \"company 1\"],\n",
    "      [\"3\", \"rohith\", \"company 2\"],\n",
    "      [\"4\", \"sridevi\", \"company 1\"],\n",
    "      [\"5\", \"bobby\", \"company 1\"]]\n",
    "col1 = ['ID', 'NAME', 'COMPANY']\n",
    "df1 = spark.createDataFrame(d1, col1)\n",
    "\n",
    "d2 = [[\"1\", \"45000\", \"IT\"],\n",
    "      [\"2\", \"145000\", \"Manager\"],\n",
    "      [\"6\", \"45000\", \"HR\"],\n",
    "      [\"5\", \"34000\", \"Sales\"]]\n",
    "col2 = ['ID', 'SALARY', 'DEPT']\n",
    "df2 = spark.createDataFrame(d2, col2)\n",
    "\n",
    "df1.join(df2, df1.ID == df2.ID, \"leftanti\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3c2bfc9-df38-4d69-a336-444f01144e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+---+------+-------+\n",
      "| ID|  NAME|  COMPANY| ID|SALARY|   DEPT|\n",
      "+---+------+---------+---+------+-------+\n",
      "|  1|sravan|company 1|  1| 45000|     IT|\n",
      "|  2|ojaswi|company 1|  2|145000|Manager|\n",
      "|  5| bobby|company 1|  5| 34000|  Sales|\n",
      "+---+------+---------+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
    "\n",
    "d1 = [[\"1\", \"sravan\", \"company 1\"],\n",
    "      [\"2\", \"ojaswi\", \"company 1\"],\n",
    "      [\"3\", \"rohith\", \"company 2\"],\n",
    "      [\"4\", \"sridevi\", \"company 1\"],\n",
    "      [\"5\", \"bobby\", \"company 1\"]]\n",
    "col1 = ['ID', 'NAME', 'COMPANY']\n",
    "df1 = spark.createDataFrame(d1, col1)\n",
    "\n",
    "d2 = [[\"1\", \"45000\", \"IT\"],\n",
    "      [\"2\", \"145000\", \"Manager\"],\n",
    "      [\"6\", \"45000\", \"HR\"],\n",
    "      [\"5\", \"34000\", \"Sales\"]]\n",
    "col2 = ['ID', 'SALARY', 'DEPT']\n",
    "df2 = spark.createDataFrame(d2, col2)\n",
    "\n",
    "# Create temp views\n",
    "df1.createOrReplaceTempView(\"student\")\n",
    "df2.createOrReplaceTempView(\"department\")\n",
    "\n",
    "# SQL join on ID\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM student\n",
    "    JOIN department\n",
    "    ON student.ID = department.ID\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e07aaa3-3c1e-4ee4-a0f7-189686d3244e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+---+------+----------+\n",
      "| ID|  NAME|  Company| ID|salary|department|\n",
      "+---+------+---------+---+------+----------+\n",
      "|  1|sravan|company 1|  1| 45000|        IT|\n",
      "|  2|ojaswi|company 1|  2|145000|   Manager|\n",
      "|  5| bobby|company 1|  5| 34000|     Sales|\n",
      "+---+------+---------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
    "\n",
    "# employee data\n",
    "data = [[\"1\", \"sravan\", \"company 1\"],\n",
    "        [\"2\", \"ojaswi\", \"company 1\"],\n",
    "        [\"3\", \"rohith\", \"company 2\"],\n",
    "        [\"4\", \"sridevi\", \"company 1\"],\n",
    "        [\"5\", \"bobby\", \"company 1\"]]\n",
    "cols = ['ID', 'NAME', 'Company']\n",
    "df1 = spark.createDataFrame(data, cols)\n",
    "\n",
    "# salary data\n",
    "data1 = [[\"1\", \"45000\", \"IT\"],\n",
    "         [\"2\", \"145000\", \"Manager\"],\n",
    "         [\"6\", \"45000\", \"HR\"],\n",
    "         [\"5\", \"34000\", \"Sales\"]]\n",
    "cols1 = ['ID', 'salary', 'department']\n",
    "df2 = spark.createDataFrame(data1, cols1)\n",
    "\n",
    "# create temp views\n",
    "df1.createOrReplaceTempView(\"student\")\n",
    "df2.createOrReplaceTempView(\"department\")\n",
    "\n",
    "# SQL inner join on ID\n",
    "spark.sql(\"SELECT * FROM student INNER JOIN department ON student.ID == department.ID\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19c58e91-3a37-4933-8e92-92f67d7b9b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+---+------+----------+\n",
      "| ID|  NAME|  Company| ID|salary|department|\n",
      "+---+------+---------+---+------+----------+\n",
      "|  1|sravan|company 1|  1| 45000|        IT|\n",
      "|  2|ojaswi|company 1|  2|145000|   Manager|\n",
      "|  5| bobby|company 1|  5| 34000|     Sales|\n",
      "+---+------+---------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
    "\n",
    "# employee data\n",
    "data = [[\"1\", \"sravan\", \"company 1\"],\n",
    "        [\"2\", \"ojaswi\", \"company 1\"],\n",
    "        [\"3\", \"rohith\", \"company 2\"],\n",
    "        [\"4\", \"sridevi\", \"company 1\"],\n",
    "        [\"5\", \"bobby\", \"company 1\"]]\n",
    "cols = ['ID', 'NAME', 'Company']\n",
    "df1 = spark.createDataFrame(data, cols)\n",
    "\n",
    "# salary data\n",
    "data1 = [[\"1\", \"45000\", \"IT\"],\n",
    "         [\"2\", \"145000\", \"Manager\"],\n",
    "         [\"6\", \"45000\", \"HR\"],\n",
    "         [\"5\", \"34000\", \"Sales\"]]\n",
    "cols1 = ['ID', 'salary', 'department']\n",
    "df2 = spark.createDataFrame(data1, cols1)\n",
    "\n",
    "# create temp views\n",
    "df1.createOrReplaceTempView(\"student\")\n",
    "df2.createOrReplaceTempView(\"department\")\n",
    "\n",
    "# SQL inner join on ID\n",
    "spark.sql(\"\"\"\n",
    "            SELECT * \n",
    "            FROM student\n",
    "            ,department \n",
    "            WHERE student.ID == department.ID\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f16109e3-7699-40b0-a181-affb399ebc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|ID1| NAME1|\n",
      "+---+------+\n",
      "|  1|sravan|\n",
      "|  2|ojsawi|\n",
      "|  3| bobby|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing module\n",
    "import pyspark\n",
    "\n",
    "# importing sparksession from pyspark.sql module\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# creating sparksession and giving an app name\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
    "\n",
    "# list  of employee data\n",
    "data = [(1, \"sravan\"), (2, \"ojsawi\"), (3, \"bobby\")]\n",
    "\n",
    "# specify column names\n",
    "columns = ['ID1', 'NAME1']\n",
    "\n",
    "# creating a dataframe from the lists of data\n",
    "dataframe = spark.createDataFrame(data, columns)\n",
    "\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ab61baf-3924-48bc-ac15-225bd26f96ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|ID2|  NAME2|\n",
      "+---+-------+\n",
      "|  1| sravan|\n",
      "|  2| ojsawi|\n",
      "|  3|  bobby|\n",
      "|  4| rohith|\n",
      "|  5|gnanesh|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing module\n",
    "import pyspark\n",
    "\n",
    "# importing sparksession from pyspark.sql module\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# creating sparksession and giving an app name\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
    "\n",
    "# list  of employee data\n",
    "data = [(1, \"sravan\"), (2, \"ojsawi\"),\n",
    "        (3, \"bobby\"),\n",
    "        (4, \"rohith\"), (5, \"gnanesh\")]\n",
    "\n",
    "# specify column names\n",
    "columns = ['ID2', 'NAME2']\n",
    "\n",
    "# creating a dataframe from the lists of data\n",
    "dataframe1 = spark.createDataFrame(data, columns)\n",
    "\n",
    "dataframe1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1527bdfc-6fe2-48a1-8aac-387beaf69612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+\n",
      "|ID1| NAME1|ID2| NAME2|\n",
      "+---+------+---+------+\n",
      "|  1|sravan|  1|sravan|\n",
      "|  2|ojsawi|  2|ojsawi|\n",
      "|  3| bobby|  3| bobby|\n",
      "+---+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing module\n",
    "import pyspark\n",
    "\n",
    "# importing sparksession from pyspark.sql module\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# creating sparksession and giving an app name\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
    "\n",
    "# list  of employee data\n",
    "data = [(1, \"sravan\"), (2, \"ojsawi\"), (3, \"bobby\")]\n",
    "\n",
    "# specify column names\n",
    "columns = ['ID1', 'NAME1']\n",
    "\n",
    "# creating a dataframe from the lists of data\n",
    "dataframe = spark.createDataFrame(data, columns)\n",
    "\n",
    "# list  of employee data\n",
    "data = [(1, \"sravan\"), (2, \"ojsawi\"), (3, \"bobby\"),\n",
    "        (4, \"rohith\"), (5, \"gnanesh\")]\n",
    "\n",
    "# specify column names\n",
    "columns = ['ID2', 'NAME2']\n",
    "\n",
    "# creating a dataframe from the lists of data\n",
    "dataframe1 = spark.createDataFrame(data, columns)\n",
    "\n",
    "# join based on ID and name column\n",
    "dataframe.join(dataframe1, (dataframe.ID1 == dataframe1.ID2)\n",
    "               & (dataframe.NAME1 == dataframe1.NAME2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b468d7ce-bf53-459f-8e0c-76fb24c02121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+\n",
      "|ID1| NAME1|ID2| NAME2|\n",
      "+---+------+---+------+\n",
      "|  1|sravan|  1|sravan|\n",
      "|  2|ojsawi|  2|ojsawi|\n",
      "|  3| bobby|  3| bobby|\n",
      "+---+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing module\n",
    "import pyspark\n",
    "\n",
    "# importing sparksession from pyspark.sql module\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# creating sparksession and giving an app name\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
    "\n",
    "# list  of employee data\n",
    "data = [(1, \"sravan\"), (2, \"ojsawi\"), (3, \"bobby\")]\n",
    "\n",
    "# specify column names\n",
    "columns = ['ID1', 'NAME1']\n",
    "\n",
    "# creating a dataframe from the lists of data\n",
    "dataframe = spark.createDataFrame(data, columns)\n",
    "\n",
    "# list  of employee data\n",
    "data = [(1, \"sravan\"), (2, \"ojsawi\"), (3, \"bobby\"),\n",
    "        (4, \"rohith\"), (5, \"gnanesh\")]\n",
    "\n",
    "# specify column names\n",
    "columns = ['ID2', 'NAME2']\n",
    "\n",
    "# creating a dataframe from the lists of data\n",
    "dataframe1 = spark.createDataFrame(data, columns)\n",
    "\n",
    "# join based on ID and name column\n",
    "dataframe.join(dataframe1, (dataframe.ID1 == dataframe1.ID2)\n",
    "               | (dataframe.NAME1 == dataframe1.NAME2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbf6ac2-b244-4856-9915-78cdb0fc65b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
