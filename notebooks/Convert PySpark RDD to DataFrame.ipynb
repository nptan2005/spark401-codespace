{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "246204a0-8a44-4562-abfe-0a09f5997f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.core.rdd.RDD'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/03 14:29:50 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def create_session():\n",
    "    spk = SparkSession.builder.appName('Çonvert Pyspark RDD to DataFrame 01').getOrCreate()\n",
    "    return spk\n",
    "\n",
    "def create_RDD(sc, data):\n",
    "    rdd = sc.parallelize(data)\n",
    "    return rdd\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_data = [(\"Uttar Pradesh\", 122000, 89600, 12238),\n",
    "                  (\"Maharashtra\", 454000, 380000, 67985),\n",
    "                  (\"Tamil Nadu\", 115000, 102000, 13933),\n",
    "                  (\"Karnataka\", 147000, 111000, 15306),\n",
    "                  (\"Kerala\", 153000, 124000, 5259)]\n",
    "\n",
    "     # calling function to create SparkSession\n",
    "    spark = create_session()\n",
    "\n",
    "    # creating spark context object\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # calling function to create RDD\n",
    "    rd_df = create_RDD(sc, input_data)\n",
    "\n",
    "    # printing the type\n",
    "    print(type(rd_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0d71a91-85a0-4837-a36b-18d2981ad1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- State: string (nullable = true)\n",
      " |-- Cases: long (nullable = true)\n",
      " |-- Recovered: long (nullable = true)\n",
      " |-- Deaths: long (nullable = true)\n",
      "\n",
      "+-------------+------+---------+------+\n",
      "|        State| Cases|Recovered|Deaths|\n",
      "+-------------+------+---------+------+\n",
      "|Uttar Pradesh|122000|    89600| 12238|\n",
      "|  Maharashtra|454000|   380000| 67985|\n",
      "|   Tamil Nadu|115000|   102000| 13933|\n",
      "|    Karnataka|147000|   111000| 15306|\n",
      "|       Kerala|153000|   124000|  5259|\n",
      "+-------------+------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def create_session():\n",
    "    spk = SparkSession.builder.appName('Çonvert Pyspark RDD to DataFrame 02').getOrCreate()\n",
    "    return spk\n",
    "\n",
    "def create_RDD(sc, data):\n",
    "    rdd = sc.parallelize(data)\n",
    "    return rdd\n",
    "\n",
    "def RDD_to_df_1(spark,df,schema):\n",
    "    df1 = spark.createDataFrame(df, schema)\n",
    "    return df1\n",
    "\n",
    "# function to convert RDD to dataframe\n",
    "def RDD_to_df_2(df,schema):\n",
    "  \n",
    "  # converting RDD to dataframe using toDF()\n",
    "  # in which we are passing schema of df\n",
    "  df2 = rd_df.toDF(schema)\n",
    "  return df2\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_data = [(\"Uttar Pradesh\",122000,89600,12238),\n",
    "          (\"Maharashtra\",454000,380000,67985),\n",
    "          (\"Tamil Nadu\",115000,102000,13933),\n",
    "          (\"Karnataka\",147000,111000,15306),\n",
    "          (\"Kerala\",153000,124000,5259)]\n",
    "\n",
    "  # calling function to create SparkSession\n",
    "    spark = create_session()\n",
    "\n",
    "  # creating spark context object\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "  # calling function to create RDD\n",
    "    rd_df = create_RDD(sc,input_data)\n",
    "\n",
    "    schema_lst = [\"State\",\"Cases\",\"Recovered\",\"Deaths\"]\n",
    "\n",
    "  # calling function to convert RDD to dataframe\n",
    "    # converted_df = RDD_to_df_1(spark,rd_df,schema_lst)\n",
    "\n",
    "    converted_df = RDD_to_df_2(rd_df,schema_lst)\n",
    "  \n",
    "  # visualizing the schema and dataframe\n",
    "    converted_df.printSchema()\n",
    "    converted_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe743fd-3a61-4c09-b1e6-7fc8977257ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
